# Ø§Ù„Ù…Ù„Ù: core/performance_optimizer.py
"""
âš¡ Ù…Ø­Ø³Ù‘Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø§Ù…Ù„ - Sky Wave ERP
================================
ÙŠÙˆÙØ± ØªØ­Ø³ÙŠÙ†Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø£Ø¯Ø§Ø¡:
- Connection Pooling Ù„Ù€ SQLite
- Query Caching Ø°ÙƒÙŠ
- Lazy Loading Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø©
- Batch Operations Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
- Memory Management
"""

import functools
import gc
import sqlite3
import threading
import time
import weakref
from collections import OrderedDict
from collections.abc import Callable
from contextlib import contextmanager
from queue import Queue
from typing import Any

from core.logger import get_logger

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¯Ø§Ù„Ø© Ø§Ù„Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¢Ù…Ù†Ø©
try:
    from core.safe_print import safe_print
except ImportError:

    def safe_print(msg):
        try:
            print(msg)
        except UnicodeEncodeError:
            pass


logger = get_logger(__name__)


class SQLiteConnectionPool:
    """
    âš¡ Connection Pool Ù„Ù€ SQLite
    ÙŠÙˆÙØ± Ø§ØªØµØ§Ù„Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ Ø¬Ø¯ÙŠØ¯ ÙƒÙ„ Ù…Ø±Ø©
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, db_path: str | None = None, pool_size: int = 5):
        if hasattr(self, "_initialized"):
            return
        self._initialized = True

        # âš¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù…Ù† Config
        if db_path is None:
            from core.config import Config

            db_path = Config.get_local_db_path()

        self.db_path = db_path
        self.pool_size = pool_size
        self._pool: Queue = Queue(maxsize=pool_size)
        self._lock = threading.RLock()
        self._active_connections = 0

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        for _ in range(pool_size):
            conn = self._create_connection()
            self._pool.put(conn)

        logger.info("âš¡ [ConnectionPool] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ %s Ø§ØªØµØ§Ù„Ø§Øª", pool_size)

    def _create_connection(self) -> sqlite3.Connection:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ Ø¬Ø¯ÙŠØ¯ Ù…Ø­Ø³Ù‘Ù†"""
        conn = sqlite3.connect(
            self.db_path,
            check_same_thread=False,
            timeout=30.0,
            isolation_level=None,  # Autocommit
        )
        conn.row_factory = sqlite3.Row

        # ØªØ­Ø³ÙŠÙ†Ø§Øª SQLite Ù„Ù„Ø£Ø¯Ø§Ø¡
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")
        conn.execute("PRAGMA cache_size=10000")
        conn.execute("PRAGMA temp_store=MEMORY")
        conn.execute("PRAGMA mmap_size=268435456")  # 256MB

        return conn

    @contextmanager
    def get_connection(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§ØªØµØ§Ù„ Ù…Ù† Ø§Ù„Ù€ pool"""
        conn = None
        try:
            conn = self._pool.get(timeout=5)
            self._active_connections += 1
            yield conn
        finally:
            if conn:
                self._active_connections -= 1
                self._pool.put(conn)

    def get_cursor(self) -> sqlite3.Cursor:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ cursor Ù…Ù†ÙØµÙ„"""
        with self._lock:
            conn = self._create_connection()
            return conn.cursor()

    def close_all(self):
        """Ø¥ØºÙ„Ø§Ù‚ ÙƒÙ„ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª"""
        while not self._pool.empty():
            try:
                conn = self._pool.get_nowait()
                conn.close()
            except Exception:
                pass
        logger.info("âš¡ [ConnectionPool] ØªÙ… Ø¥ØºÙ„Ø§Ù‚ ÙƒÙ„ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª")


class SmartQueryCache:
    """
    âš¡ Cache Ø°ÙƒÙŠ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
    - TTL (Time To Live) Ù„ÙƒÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù…
    - LRU (Least Recently Used) Ù„Ù„ØªÙ†Ø¸ÙŠÙ
    - Invalidation Ø°ÙƒÙŠ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ø¯ÙˆÙ„
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, max_size: int = 500, default_ttl: int = 60):
        if hasattr(self, "_initialized"):
            return
        self._initialized = True

        self.max_size = max_size
        self.default_ttl = default_ttl
        self._cache: OrderedDict = OrderedDict()
        self._timestamps: dict[str, float] = {}
        self._table_keys: dict[str, set] = {}  # table -> set of cache keys
        self._lock = threading.RLock()

        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self._hits = 0
        self._misses = 0

        logger.info("âš¡ [QueryCache] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ cache Ø¨Ø­Ø¬Ù… %s", max_size)

    def get(self, key: str) -> Any | None:
        """Ø¬Ù„Ø¨ Ù‚ÙŠÙ…Ø© Ù…Ù† Ø§Ù„Ù€ cache"""
        with self._lock:
            if key not in self._cache:
                self._misses += 1
                return None

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
            if time.time() - self._timestamps[key] > self.default_ttl:
                self._remove_key(key)
                self._misses += 1
                return None

            # Ù†Ù‚Ù„ Ù„Ù„Ù†Ù‡Ø§ÙŠØ© (Ø§Ù„Ø£Ø­Ø¯Ø«)
            self._cache.move_to_end(key)
            self._hits += 1
            return self._cache[key]

    def set(self, key: str, value: Any, table: str | None = None, ttl: int | None = None):
        """ØªØ®Ø²ÙŠÙ† Ù‚ÙŠÙ…Ø© ÙÙŠ Ø§Ù„Ù€ cache"""
        with self._lock:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù‚Ø¯Ù… Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
            while len(self._cache) >= self.max_size:
                oldest_key = next(iter(self._cache))
                self._remove_key(oldest_key)

            self._cache[key] = value
            self._timestamps[key] = time.time()

            # Ø±Ø¨Ø· Ø§Ù„Ù…ÙØªØ§Ø­ Ø¨Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ù„Ù„Ù€ invalidation
            if table:
                if table not in self._table_keys:
                    self._table_keys[table] = set()
                self._table_keys[table].add(key)

    def _remove_key(self, key: str):
        """Ø¥Ø²Ø§Ù„Ø© Ù…ÙØªØ§Ø­ Ù…Ù† Ø§Ù„Ù€ cache"""
        self._cache.pop(key, None)
        self._timestamps.pop(key, None)
        # Ø¥Ø²Ø§Ù„Ø© Ù…Ù† table_keys
        for table_keys in self._table_keys.values():
            table_keys.discard(key)

    def invalidate_table(self, table: str):
        """Ø¥Ø¨Ø·Ø§Ù„ ÙƒÙ„ Ø§Ù„Ù€ cache Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¨Ø¬Ø¯ÙˆÙ„ Ù…Ø¹ÙŠÙ†"""
        with self._lock:
            if table in self._table_keys:
                for key in list(self._table_keys[table]):
                    self._remove_key(key)
                self._table_keys[table].clear()
                logger.debug("âš¡ [QueryCache] ØªÙ… Ø¥Ø¨Ø·Ø§Ù„ cache Ø§Ù„Ø¬Ø¯ÙˆÙ„: %s", table)

    def invalidate_all(self):
        """Ø¥Ø¨Ø·Ø§Ù„ ÙƒÙ„ Ø§Ù„Ù€ cache"""
        with self._lock:
            self._cache.clear()
            self._timestamps.clear()
            self._table_keys.clear()
            logger.info("âš¡ [QueryCache] ØªÙ… Ø¥Ø¨Ø·Ø§Ù„ ÙƒÙ„ Ø§Ù„Ù€ cache")

    def get_stats(self) -> dict:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù€ cache"""
        total = self._hits + self._misses
        hit_rate = (self._hits / total * 100) if total > 0 else 0
        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "hits": self._hits,
            "misses": self._misses,
            "hit_rate": f"{hit_rate:.1f}%",
        }


class BatchProcessor:
    """
    âš¡ Ù…Ø¹Ø§Ù„Ø¬ Ø¯ÙØ¹Ø§Øª Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
    ÙŠØ¬Ù…Ø¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ÙˆÙŠÙÙ†ÙØ°Ù‡Ø§ Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© Ù„Ù„Ø£Ø¯Ø§Ø¡
    """

    def __init__(self, batch_size: int = 100, flush_interval: float = 1.0):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self._queue: list[tuple] = []
        self._lock = threading.Lock()
        self._last_flush = time.time()
        self._callbacks: list[Callable] = []

    def add(self, operation: str, table: str, data: dict):
        """Ø¥Ø¶Ø§ÙØ© Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ø¯ÙØ¹Ø©"""
        with self._lock:
            self._queue.append((operation, table, data))

            # Flush Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ø£Ùˆ Ù…Ø± Ø§Ù„ÙˆÙ‚Øª
            if len(self._queue) >= self.batch_size:
                self._flush()
            elif time.time() - self._last_flush > self.flush_interval:
                self._flush()

    def _flush(self):
        """ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯ÙØ¹Ø©"""
        if not self._queue:
            return

        batch = self._queue.copy()
        self._queue.clear()
        self._last_flush = time.time()

        # ØªÙ†ÙÙŠØ° callbacks
        for callback in self._callbacks:
            try:
                callback(batch)
            except Exception as e:
                logger.error("âš¡ [BatchProcessor] Ø®Ø·Ø£ ÙÙŠ callback: %s", e)

    def on_flush(self, callback: Callable):
        """Ø¥Ø¶Ø§ÙØ© callback Ø¹Ù†Ø¯ Ø§Ù„Ù€ flush"""
        self._callbacks.append(callback)

    def force_flush(self):
        """Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„Ù€ flush"""
        with self._lock:
            self._flush()


class MemoryManager:
    """
    âš¡ Ù…Ø¯ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    ÙŠØ±Ø§Ù‚Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆÙŠÙÙ†Ø¸ÙÙ‡Ø§ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
    """

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if hasattr(self, "_initialized"):
            return
        self._initialized = True

        self._weak_refs: list[weakref.ref] = []
        self._cleanup_threshold = 100 * 1024 * 1024  # 100MB

    def register_disposable(self, obj):
        """ØªØ³Ø¬ÙŠÙ„ ÙƒØ§Ø¦Ù† Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙ†Ø¸ÙŠÙ"""
        self._weak_refs.append(weakref.ref(obj))

    def cleanup(self, force: bool = False):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù€ weak refs Ø§Ù„Ù…ÙŠØªØ©
        self._weak_refs = [ref for ref in self._weak_refs if ref() is not None]

        # ØªØ´ØºÙŠÙ„ garbage collector
        collected = gc.collect()

        if collected > 0:
            logger.debug("âš¡ [MemoryManager] ØªÙ… ØªÙ†Ø¸ÙŠÙ %s ÙƒØ§Ø¦Ù†", collected)

        return collected

    def get_memory_usage(self) -> dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""

        return {
            "gc_objects": len(gc.get_objects()),
            "gc_garbage": len(gc.garbage),
            "weak_refs": len(self._weak_refs),
        }


# ==================== Decorators ====================


def cached_query(table: str, ttl: int = 60):
    """
    âš¡ Decorator Ù„ØªØ®Ø²ÙŠÙ† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª

    @cached_query("projects", ttl=120)
    def get_all_projects():
        ...
    """

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            cache = SmartQueryCache()

            # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ ÙØ±ÙŠØ¯
            key = f"{func.__name__}:{hash(str(args))}:{hash(str(sorted(kwargs.items())))}"

            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù…Ù† Ø§Ù„Ù€ cache
            result = cache.get(key)
            if result is not None:
                return result

            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ø§Ù„Ø©
            result = func(*args, **kwargs)

            # ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„Ù€ cache
            cache.set(key, result, table=table, ttl=ttl)

            return result

        # Ø¥Ø¶Ø§ÙØ© Ø¯Ø§Ù„Ø© Ù„Ø¥Ø¨Ø·Ø§Ù„ Ø§Ù„Ù€ cache
        wrapper.invalidate = lambda: SmartQueryCache().invalidate_table(table)
        return wrapper

    return decorator


def batch_operation(batch_size: int = 50):
    """
    âš¡ Decorator Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ÙÙŠ Ø¯ÙØ¹Ø§Øª
    """

    def decorator(func: Callable) -> Callable:
        processor = BatchProcessor(batch_size=batch_size)

        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # ØªÙ†ÙÙŠØ° Ù…Ø¨Ø§Ø´Ø± Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ©
            return func(*args, **kwargs)

        wrapper.batch_add = processor.add
        wrapper.batch_flush = processor.force_flush
        return wrapper

    return decorator


def measure_time(func: Callable) -> Callable:
    """
    âš¡ Decorator Ù„Ù‚ÙŠØ§Ø³ ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°
    """

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = time.perf_counter() - start

        if elapsed > 0.5:  # ØªØ­Ø°ÙŠØ± Ø¥Ø°Ø§ Ø£ÙƒØ«Ø± Ù…Ù† 500ms
            logger.warning("âš ï¸ [Performance] %s Ø§Ø³ØªØºØ±Ù‚ %.2fs", func.__name__, elapsed)

        return result

    return wrapper


# ==================== Utility Functions ====================


def optimize_sqlite_connection(conn: sqlite3.Connection):
    """ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª SQLite Ø¹Ù„Ù‰ Ø§ØªØµØ§Ù„ Ù…ÙˆØ¬ÙˆØ¯"""
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA cache_size=10000")
    conn.execute("PRAGMA temp_store=MEMORY")
    conn.execute("PRAGMA mmap_size=268435456")


def get_query_cache() -> SmartQueryCache:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ instance Ù…Ù† QueryCache"""
    return SmartQueryCache()


def get_memory_manager() -> MemoryManager:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ instance Ù…Ù† MemoryManager"""
    return MemoryManager()


def invalidate_all_caches():
    """Ø¥Ø¨Ø·Ø§Ù„ ÙƒÙ„ Ø§Ù„Ù€ caches"""
    SmartQueryCache().invalidate_all()
    logger.info("âš¡ ØªÙ… Ø¥Ø¨Ø·Ø§Ù„ ÙƒÙ„ Ø§Ù„Ù€ caches")


def print_performance_stats():
    """Ø·Ø¨Ø§Ø¹Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    cache = SmartQueryCache()
    memory = MemoryManager()

    safe_print("\n" + "=" * 60)
    safe_print("âš¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ - Sky Wave ERP")
    safe_print("=" * 60)
    safe_print(f"ğŸ“Š Query Cache: {cache.get_stats()}")
    safe_print(f"ğŸ’¾ Memory: {memory.get_memory_usage()}")
    safe_print("=" * 60 + "\n")


# ==================== Main Performance Optimizer Class ====================


class PerformanceOptimizer:
    """
    âš¡ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…Ø­Ø³Ù‘Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡
    ØªØ¬Ù…Ø¹ ÙƒÙ„ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† ÙÙŠ Ù…ÙƒØ§Ù† ÙˆØ§Ø­Ø¯
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if hasattr(self, "_initialized"):
            return
        self._initialized = True

        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self.connection_pool = SQLiteConnectionPool()
        self.query_cache = SmartQueryCache()
        self.memory_manager = MemoryManager()
        self.batch_processor = BatchProcessor()

        logger.info("âš¡ [PerformanceOptimizer] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø³Ù‘Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡")

    def get_connection(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§ØªØµØ§Ù„ Ù…Ø­Ø³Ù‘Ù†"""
        return self.connection_pool.get_connection()

    def cache_query(self, key: str, value: Any, table: str = None, ttl: int = 60):
        """ØªØ®Ø²ÙŠÙ† Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ Ø§Ù„Ù€ cache"""
        self.query_cache.set(key, value, table, ttl)

    def get_cached_query(self, key: str):
        """Ø¬Ù„Ø¨ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù† Ø§Ù„Ù€ cache"""
        return self.query_cache.get(key)

    def invalidate_table_cache(self, table: str):
        """Ø¥Ø¨Ø·Ø§Ù„ cache Ø¬Ø¯ÙˆÙ„ Ù…Ø¹ÙŠÙ†"""
        self.query_cache.invalidate_table(table)

    def cleanup_memory(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        return self.memory_manager.cleanup()

    def get_stats(self) -> dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        return {
            "cache": self.query_cache.get_stats(),
            "memory": self.memory_manager.get_memory_usage(),
            "connections": {
                "active": self.connection_pool._active_connections,
                "pool_size": self.connection_pool.pool_size,
            },
        }

    def print_stats(self):
        """Ø·Ø¨Ø§Ø¹Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        stats = self.get_stats()
        safe_print("\n" + "=" * 60)
        safe_print("âš¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ø­Ø³Ù‘Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ - Sky Wave ERP")
        safe_print("=" * 60)
        safe_print(f"ğŸ“Š Query Cache: {stats['cache']}")
        safe_print(f"ğŸ’¾ Memory: {stats['memory']}")
        safe_print(f"ğŸ”— Connections: {stats['connections']}")
        safe_print("=" * 60 + "\n")


# ==================== Global Instance ====================

# Ø¥Ù†Ø´Ø§Ø¡ instance Ø¹Ø§Ù… Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
_OPTIMIZER = None


def get_performance_optimizer() -> PerformanceOptimizer:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ instance Ù…Ù† PerformanceOptimizer"""
    global _OPTIMIZER
    if _OPTIMIZER is None:
        _OPTIMIZER = PerformanceOptimizer()
    return _OPTIMIZER


# ØªØµØ¯ÙŠØ± Ø§Ù„ÙØ¦Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©
__all__ = [
    "PerformanceOptimizer",
    "SQLiteConnectionPool",
    "SmartQueryCache",
    "BatchProcessor",
    "MemoryManager",
    "cached_query",
    "batch_operation",
    "measure_time",
    "get_performance_optimizer",
    "get_query_cache",
    "get_memory_manager",
    "invalidate_all_caches",
    "print_performance_stats",
]
