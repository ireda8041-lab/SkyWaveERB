# Ø§Ù„Ù…Ù„Ù: core/unified_sync.py
"""
ğŸ”„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…ÙˆØ­Ø¯ - MongoDB First
MongoDB Ù‡Ùˆ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØŒ SQLite Ù†Ø³Ø®Ø© Ù…Ø­Ù„ÙŠØ© Ù„Ù„Ù€ offline ÙÙ‚Ø·

Ø§Ù„Ù…Ø¨Ø¯Ø£:
- Ø¹Ù†Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„: MongoDB = Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©
- Ø¹Ù†Ø¯ Ø¹Ø¯Ù… Ø§Ù„Ø§ØªØµØ§Ù„: SQLite ÙŠØ­ÙØ¸ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ù…Ø¤Ù‚ØªØ§Ù‹
- Ø¹Ù†Ø¯ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„: Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø«Ù… Ù…Ø³Ø­ ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ù…Ù† MongoDB
"""

import hashlib
import json
import os
import platform
import threading
import time
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from PyQt6.QtCore import QObject, QTimer, pyqtSignal

from core.logger import get_logger

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¯Ø§Ù„Ø© Ø§Ù„Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¢Ù…Ù†Ø©
try:
    from core.safe_print import safe_print
except ImportError:

    def safe_print(msg):
        try:
            print(msg)
        except UnicodeEncodeError:
            pass


logger = get_logger(__name__)


def _get_stable_device_id() -> str:
    """Return a stable device id used for cross-device sync pings."""
    try:
        machine_info = f"{platform.node()}-{platform.machine()}-{platform.processor()}"
        try:
            digest = hashlib.md5(machine_info.encode(), usedforsecurity=False).hexdigest()
        except TypeError:
            digest = hashlib.sha256(machine_info.encode()).hexdigest()
        return digest[:8]
    except Exception:
        device_file = os.path.join(os.path.expanduser("~"), ".skywave_device_id")
        if os.path.exists(device_file):
            try:
                with open(device_file, encoding="utf-8") as f:
                    return f.read().strip()
            except Exception:
                pass
        new_id = str(uuid.uuid4())[:8]
        try:
            with open(device_file, "w", encoding="utf-8") as f:
                f.write(new_id)
        except OSError:
            pass
        return new_id


# ==================== Ø«ÙˆØ§Ø¨Øª Ø§Ù„ØªÙˆÙ‚ÙŠØª (Ø¨Ø§Ù„Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©) ====================
FULL_SYNC_INTERVAL_MS = 15 * 60 * 1000
QUICK_SYNC_INTERVAL_MS = 3 * 60 * 1000
CONNECTION_CHECK_INTERVAL_MS = 90 * 1000
CLOUD_PULL_INTERVAL_MS = 45 * 1000
DEFAULT_DELTA_SYNC_INTERVAL_SECONDS = 2
DEFAULT_REALTIME_CHANGE_STREAM_MAX_AWAIT_MS = 250
DEFAULT_LAZY_LOGO_ENABLED = True
DEFAULT_LOGO_FETCH_BATCH_LIMIT = 10


class UnifiedSyncManagerV3(QObject):
    """
    Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…ÙˆØ­Ø¯ - MongoDB First Architecture
    Ù…Ø¹ Ù†Ø¸Ø§Ù… Ù…Ø²Ø§Ù…Ù†Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ø§Ø­ØªØ±Ø§ÙÙŠ
    """

    # Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª
    sync_started = pyqtSignal()
    sync_progress = pyqtSignal(str, int, int)  # table, current, total
    sync_completed = pyqtSignal(dict)
    sync_error = pyqtSignal(str)
    connection_changed = pyqtSignal(bool)  # online/offline
    data_synced = pyqtSignal()  # âš¡ NEW: Signal emitted after successful pull for UI refresh

    # Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
    TABLES = [
        "accounts",
        "clients",
        "services",
        "projects",
        "invoices",
        "payments",
        "expenses",
        "journal_entries",
        "currencies",
        "notifications",
        "tasks",
    ]

    # Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù„ÙƒÙ„ Ø¬Ø¯ÙˆÙ„
    UNIQUE_FIELDS = {
        "clients": "name",
        "projects": "name",
        "services": "name",
        "accounts": "code",
        "invoices": "invoice_number",
        "payments": "id",
        "expenses": "id",
        "journal_entries": "id",
        "currencies": "code",
        "users": "username",
        "notifications": "id",
        "tasks": "id",
    }

    def __init__(self, repository, parent=None):
        super().__init__(parent)
        self.repo = repository
        self._lock = threading.RLock()
        self._delta_cycle_lock = threading.Lock()
        self._is_syncing = False
        self._max_retries = 3
        self._last_online_status = None
        self._shutdown = False  # âš¡ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ø¥ØºÙ„Ø§Ù‚
        self._last_full_sync_at = None
        self._sync_metrics_lock = threading.RLock()
        self._sync_metrics = {
            "total_syncs": 0,
            "successful_syncs": 0,
            "failed_syncs": 0,
            "last_sync_time": None,
            "total_records_synced": 0,
        }

        # âš¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© - Ù…ÙØ¹Ù‘Ù„Ø© Ù„Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©
        self._auto_sync_enabled = True
        self._auto_sync_interval = FULL_SYNC_INTERVAL_MS
        self._quick_sync_interval = QUICK_SYNC_INTERVAL_MS
        self._connection_check_interval = CONNECTION_CHECK_INTERVAL_MS
        self._delta_sync_interval_seconds = DEFAULT_DELTA_SYNC_INTERVAL_SECONDS
        self._realtime_enabled = True
        self._realtime_auto_detect = True
        self._realtime_change_stream_max_await_ms = DEFAULT_REALTIME_CHANGE_STREAM_MAX_AWAIT_MS
        self._lazy_logo_enabled = DEFAULT_LAZY_LOGO_ENABLED
        self._logo_fetch_batch_limit = DEFAULT_LOGO_FETCH_BATCH_LIMIT
        self._realtime_pull_dedupe_ms = 400
        self._last_realtime_pull_ms: dict[str, int] = {}
        self._queued_realtime_tables: set[str] = set()
        self._instant_sync_schedule_lock = threading.Lock()
        self._instant_sync_pending_tables: set[str] = set()
        self._instant_sync_worker_running = False
        self._instant_sync_dedupe_ms = 250
        self._last_instant_sync_request_ms: dict[str, int] = {}
        self._device_id = _get_stable_device_id()
        self._last_sync_ping_at: dict[str, float] = {}

        # âš¡ Ø§Ù„Ù…Ø¤Ù‚ØªØ§Øª
        self._auto_sync_timer = None
        self._quick_sync_timer = None
        self._connection_timer = None
        self._cloud_pull_timer = None
        self._delta_pull_timer = None  # âš¡ NEW: Ù…Ø¤Ù‚Øª Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„ØªÙØ§Ø¶Ù„ÙŠ
        self._delta_thread = None
        self._delta_thread_stop = threading.Event()

        self._load_sync_config()

        # âš¡ Watermarks Ù„Ù„Ù€ Delta Sync
        self._watermarks: dict[str, str] = {}
        self._load_watermarks()

        logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© UnifiedSyncManager - Ù…Ø²Ø§Ù…Ù†Ø© Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ø£Ø¯Ø§Ø¡")

    def _emit_sync_pings(self, tables: set[str]) -> None:
        if not tables:
            return
        if not self.is_online or self.repo is None or self.repo.mongo_db is None:
            return
        collection = self.repo.mongo_db["notifications"]
        now_iso = datetime.now().isoformat()
        now_ts = time.time()
        for table in tables:
            # Avoid notification table echo storms.
            if table == "notifications":
                continue
            last_ping = self._last_sync_ping_at.get(table, 0.0)
            if (now_ts - last_ping) < 0.6:
                continue
            self._last_sync_ping_at[table] = now_ts
            payload = {
                "message": f"sync ping: {table}",
                "type": "info",
                "title": "sync",
                "device_id": self._device_id,
                "created_at": now_iso,
                "entity_type": table,
                "action": "sync_ping",
                "silent": True,
            }
            try:
                collection.insert_one(payload)
            except Exception as e:
                logger.debug("ØªØ¹Ø°Ø± Ø¥Ø±Ø³Ø§Ù„ sync ping Ù„Ù€ %s: %s", table, e)

    def _update_sync_metrics(self, success: bool, records_synced: int = 0):
        """Update lightweight sync counters used by settings UI."""
        try:
            with self._sync_metrics_lock:
                self._sync_metrics["total_syncs"] += 1
                if success:
                    self._sync_metrics["successful_syncs"] += 1
                    self._sync_metrics["total_records_synced"] += max(0, int(records_synced))
                else:
                    self._sync_metrics["failed_syncs"] += 1
                self._sync_metrics["last_sync_time"] = datetime.now().isoformat()
        except Exception:
            # Metrics are non-critical.
            pass

    @staticmethod
    def _safe_int(value: Any, default: int, minimum: int = 1, maximum: int | None = None) -> int:
        """Convert a value to int with sane bounds."""
        try:
            parsed = int(value)
        except (TypeError, ValueError):
            parsed = int(default)
        if parsed < minimum:
            parsed = minimum
        if maximum is not None and parsed > maximum:
            parsed = maximum
        return parsed

    def _load_sync_config(self):
        """Load sync intervals from sync_config.json if available."""
        try:
            config_path = Path("sync_config.json")
            if not config_path.exists():
                return

            with open(config_path, encoding="utf-8") as f:
                config = json.load(f)

            self._auto_sync_enabled = bool(config.get("enabled", self._auto_sync_enabled))

            default_auto_seconds = max(1, self._auto_sync_interval // 1000)
            default_quick_seconds = max(1, self._quick_sync_interval // 1000)
            default_connection_seconds = max(1, self._connection_check_interval // 1000)

            auto_seconds = self._safe_int(
                config.get("auto_sync_interval", default_auto_seconds),
                default_auto_seconds,
                minimum=30,
                maximum=3600,
            )
            quick_seconds = self._safe_int(
                config.get("quick_sync_interval", default_quick_seconds),
                default_quick_seconds,
                minimum=1,
                maximum=300,
            )
            connection_seconds = self._safe_int(
                config.get("connection_check_interval", default_connection_seconds),
                default_connection_seconds,
                minimum=1,
                maximum=300,
            )
            delta_seconds = self._safe_int(
                config.get("delta_sync_interval", DEFAULT_DELTA_SYNC_INTERVAL_SECONDS),
                DEFAULT_DELTA_SYNC_INTERVAL_SECONDS,
                minimum=1,
                maximum=300,
            )
            realtime_max_await_ms = self._safe_int(
                config.get(
                    "realtime_change_stream_max_await_ms",
                    DEFAULT_REALTIME_CHANGE_STREAM_MAX_AWAIT_MS,
                ),
                DEFAULT_REALTIME_CHANGE_STREAM_MAX_AWAIT_MS,
                minimum=50,
                maximum=5000,
            )
            logo_fetch_batch_limit = self._safe_int(
                config.get("logo_fetch_batch_limit", DEFAULT_LOGO_FETCH_BATCH_LIMIT),
                DEFAULT_LOGO_FETCH_BATCH_LIMIT,
                minimum=1,
                maximum=100,
            )

            self._auto_sync_interval = auto_seconds * 1000
            self._quick_sync_interval = quick_seconds * 1000
            self._connection_check_interval = connection_seconds * 1000
            self._delta_sync_interval_seconds = delta_seconds
            self._realtime_enabled = bool(config.get("realtime_enabled", self._realtime_enabled))
            self._realtime_auto_detect = bool(
                config.get("realtime_auto_detect", self._realtime_auto_detect)
            )
            self._realtime_change_stream_max_await_ms = realtime_max_await_ms
            self._lazy_logo_enabled = bool(
                config.get("lazy_logo_enabled", DEFAULT_LAZY_LOGO_ENABLED)
            )
            self._logo_fetch_batch_limit = logo_fetch_batch_limit

            logger.info(
                "âš™ï¸ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: full=%ss quick=%ss connection=%ss delta=%ss enabled=%s realtime=%s lazy_logo=%s",
                auto_seconds,
                quick_seconds,
                connection_seconds,
                delta_seconds,
                self._auto_sync_enabled,
                self._realtime_enabled,
                self._lazy_logo_enabled,
            )
        except Exception as e:
            logger.debug("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ sync_config.json: %s", e)

    def _check_mongodb_connection(self) -> bool:
        try:
            if not self.is_online:
                return False
            if self.repo.mongo_db is None or self.repo.mongo_client is None:
                logger.warning("MongoDB client Ø£Ùˆ database ØºÙŠØ± Ù…ØªÙˆÙØ±")
                return False
            self.repo.mongo_client.admin.command("ping", maxTimeMS=5000)
            server_info = self.repo.mongo_client.server_info()
            if not server_info:
                logger.warning("ÙØ´Ù„ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø§Ø¯Ù…")
                return False
            return True
        except Exception as e:
            error_msg = str(e).lower()
            if "cannot use mongoclient after close" in error_msg:
                logger.debug("MongoDB client Ù…ØºÙ„Ù‚")
            elif "serverselectiontimeout" in error_msg:
                logger.debug("Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ MongoDB")
            elif "network" in error_msg or "connection" in error_msg:
                logger.debug("Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ù…Ø¹ MongoDB")
            else:
                logger.warning("Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ MongoDB: %s", e)
            return False

    def _safe_mongodb_operation(self, operation_func, *args, **kwargs):
        try:
            if not self._check_mongodb_connection():
                return None
            return operation_func(*args, **kwargs)
        except Exception as e:
            logger.error("ÙØ´Ù„ Ø¹Ù…Ù„ÙŠØ© MongoDB: %s", e, exc_info=True)
            return None

    @staticmethod
    def _is_closed_sqlite_error(exc: Exception) -> bool:
        return "closed database" in str(exc).lower()

    @staticmethod
    def _to_iso_timestamp(value: Any) -> str:
        """Normalize timestamp values from Mongo/SQLite to ISO string."""
        if value is None:
            return ""
        if hasattr(value, "isoformat"):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
        return str(value)

    @staticmethod
    def _parse_iso_datetime(value: Any) -> datetime | None:
        """Parse ISO timestamp string safely."""
        if not isinstance(value, str) or not value.strip():
            return None
        try:
            return datetime.fromisoformat(value.replace("Z", "+00:00"))
        except ValueError:
            return None

    def _build_last_modified_query(self, watermark: str) -> dict[str, Any]:
        """Build query that handles mixed datetime/string storage in MongoDB."""
        conditions: list[dict[str, Any]] = [{"last_modified": {"$gt": watermark}}]
        watermark_dt = self._parse_iso_datetime(watermark)
        if watermark_dt is not None:
            conditions.append({"last_modified": {"$gt": watermark_dt}})
        return conditions[0] if len(conditions) == 1 else {"$or": conditions}

    @staticmethod
    def _format_bytes(value: int) -> str:
        size = float(max(0, int(value)))
        units = ["B", "KB", "MB", "GB"]
        unit_idx = 0
        while size >= 1024 and unit_idx < (len(units) - 1):
            size /= 1024.0
            unit_idx += 1
        return f"{size:.1f}{units[unit_idx]}"

    def request_realtime_pull(self, table: str | None = None) -> bool:
        """
        Trigger a guarded delta pull in response to realtime events.
        Returns True if a pull was scheduled, False if deduplicated/ignored.
        """
        table_key = table or "__all__"
        now_ms = int(time.monotonic() * 1000)
        last_ms = self._last_realtime_pull_ms.get(table_key, 0)
        if (now_ms - last_ms) < self._realtime_pull_dedupe_ms:
            return False
        self._last_realtime_pull_ms[table_key] = now_ms

        if self._shutdown:
            return False

        if self._is_syncing:
            self._queued_realtime_tables.add(table_key)
            return False

        if not self.is_online:
            return False

        self.force_pull(table if table in self.TABLES else None)
        return True

    def _flush_realtime_pull_queue(self):
        """Run one pull cycle after full sync if realtime events arrived during lock."""
        if self._shutdown or not self._queued_realtime_tables:
            return
        queued = sorted(self._queued_realtime_tables)
        self._queued_realtime_tables.clear()
        logger.debug("âš¡ Ù…Ø¹Ø§Ù„Ø¬Ø© queued realtime events: %s", queued)
        self.force_pull(None)

    def _normalize_table_key(self, table: str | None) -> str:
        if isinstance(table, str) and table in self.TABLES:
            return table
        return "__all__"

    def schedule_instant_sync(self, table: str | None = None) -> bool:
        """
        Schedule a non-blocking instant sync cycle.
        Uses lightweight delta cycle (push+pull) with burst dedupe to avoid UI freezes.
        """
        if self._shutdown:
            return False

        table_key = self._normalize_table_key(table)
        now_ms = int(time.monotonic() * 1000)

        with self._instant_sync_schedule_lock:
            last_ms = self._last_instant_sync_request_ms.get(table_key, 0)
            if (now_ms - last_ms) < self._instant_sync_dedupe_ms:
                return False
            self._last_instant_sync_request_ms[table_key] = now_ms
            self._instant_sync_pending_tables.add(table_key)

            if self._instant_sync_worker_running:
                return True
            self._instant_sync_worker_running = True

        threading.Thread(
            target=self._instant_sync_worker_loop,
            daemon=True,
            name="unified-instant-sync",
        ).start()
        return True

    def _instant_sync_worker_loop(self):
        """Drain scheduled instant-sync requests in background without blocking UI thread."""
        try:
            # Short batching window to collapse rapid CRUD bursts into one cycle.
            time.sleep(0.06)
            while not self._shutdown:
                with self._instant_sync_schedule_lock:
                    pending = set(self._instant_sync_pending_tables)
                    self._instant_sync_pending_tables.clear()

                if not pending:
                    break

                if not self.is_online:
                    # Offline mode: skip immediate cycle; periodic sync will recover on reconnect.
                    continue

                result = self._run_delta_cycle()
                if result.get("reason") in {"delta_busy", "full_sync_in_progress"}:
                    with self._instant_sync_schedule_lock:
                        self._instant_sync_pending_tables.update(pending)
                    time.sleep(0.15)
                    continue

                # Keep loop cooperative and allow new pending requests to be batched.
                time.sleep(0.02)
        finally:
            restart_worker = False
            with self._instant_sync_schedule_lock:
                self._instant_sync_worker_running = False
                if self._instant_sync_pending_tables and not self._shutdown:
                    self._instant_sync_worker_running = True
                    restart_worker = True
            if restart_worker:
                threading.Thread(
                    target=self._instant_sync_worker_loop,
                    daemon=True,
                    name="unified-instant-sync",
                ).start()

    # ==========================================
    # ğŸš€ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙÙˆØ±ÙŠØ© - Real-time Sync
    # ==========================================

    def instant_sync(self, table: str = None):
        """
        âš¡ Ù…Ø²Ø§Ù…Ù†Ø© ÙÙˆØ±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯ Ø£Ùˆ ÙƒÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„

        Args:
            table: Ø§Ø³Ù… Ø§Ù„Ø¬Ø¯ÙˆÙ„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ). Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ­Ø¯Ø¯ØŒ ÙŠØªÙ… Ù…Ø²Ø§Ù…Ù†Ø© ÙƒÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        """
        if self._shutdown or not self.is_online:
            return

        if self._is_syncing:
            if table:
                table_key = table if table in self.TABLES else "__all__"
                self._queued_realtime_tables.add(table_key)
            return

        if table:
            table_key = table if table in self.TABLES else "__all__"
            now_ms = int(time.monotonic() * 1000)
            last_ms = self._last_realtime_pull_ms.get(table_key, 0)
            if (now_ms - last_ms) < self._realtime_pull_dedupe_ms:
                return
            self._last_realtime_pull_ms[table_key] = now_ms

        try:
            with self._lock:
                if table:
                    # Ø¨Ø¹Ø¶ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª (Ù…Ø«Ù„ accounting) Ù„ÙŠØ³Øª Ø£Ø³Ù…Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ ÙØ¹Ù„ÙŠØ©.
                    if table not in self.TABLES:
                        self._push_pending_changes()
                        logger.debug("âš¡ Ù…Ø²Ø§Ù…Ù†Ø© ÙÙˆØ±ÙŠØ© Ø¹Ø§Ù…Ø© Ø¨Ø³Ø¨Ø¨ Ø¥Ø´Ø§Ø±Ø© %s", table)
                        return

                    # Ù…Ø²Ø§Ù…Ù†Ø© Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯
                    self._sync_single_table_to_cloud(table)
                    self._sync_single_table_from_cloud(table)
                    logger.debug("âš¡ ØªÙ… Ù…Ø²Ø§Ù…Ù†Ø© %s ÙÙˆØ±Ø§Ù‹", table)
                else:
                    self._push_pending_changes()
                    for table_name in self.TABLES:
                        self._sync_single_table_from_cloud(table_name)
                    logger.debug("âš¡ ØªÙ… Ù…Ø²Ø§Ù…Ù†Ø© ÙƒÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙÙˆØ±Ø§Ù‹")
        except Exception as e:
            logger.debug("Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙÙˆØ±ÙŠØ©: %s", e)

    def _sync_single_table_from_cloud(self, table: str):
        """Ù…Ø²Ø§Ù…Ù†Ø© Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©"""
        if not self.is_online or self.repo is None or self.repo.mongo_db is None:
            return

        if table not in self.TABLES:
            return

        try:
            self._sync_table_from_cloud(table)
        except Exception as e:
            logger.debug("Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© %s Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©: %s", table, e)

    def _sync_single_table_to_cloud(self, table: str):
        """Ù…Ø²Ø§Ù…Ù†Ø© Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯ ÙÙˆØ±Ø§Ù‹"""
        if not self.is_online or self.repo is None or self.repo.mongo_db is None:
            return

        # âš¡ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
        if table not in self.TABLES:
            return

        try:
            # âš¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… cursor Ù…Ù†ÙØµÙ„ Ù„ØªØ¬Ù†Ø¨ Recursive cursor error
            cursor = self.repo.get_cursor()
            try:
                # âš¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø£ÙˆÙ„Ø§Ù‹
                cursor.execute(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table,)
                )
                if not cursor.fetchone():
                    return  # Ø§Ù„Ø¬Ø¯ÙˆÙ„ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯

                cursor.execute(
                    f"SELECT * FROM {table} WHERE sync_status != 'synced' OR sync_status IS NULL"
                )
                rows = cursor.fetchall()

                if not rows:
                    return

                columns = [desc[0] for desc in cursor.description]
                collection = self.repo.mongo_db[table]

                updated_any = False
                for row in rows:
                    record = dict(zip(columns, row, strict=False))
                    mongo_id = record.get("_mongo_id")

                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    clean_record = {
                        k: v
                        for k, v in record.items()
                        if k not in ["id", "sync_status", "last_synced"]
                    }

                    if mongo_id:
                        # ØªØ­Ø¯ÙŠØ«
                        from bson import ObjectId

                        collection.update_one(
                            {"_id": ObjectId(mongo_id)}, {"$set": clean_record}, upsert=True
                        )
                    else:
                        # Ø¥Ø¶Ø§ÙØ© Ø¬Ø¯ÙŠØ¯
                        result = collection.insert_one(clean_record)
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ mongo_id Ù…Ø­Ù„ÙŠØ§Ù‹
                        cursor.execute(
                            f"UPDATE {table} SET _mongo_id = ?, sync_status = 'synced' WHERE id = ?",
                            (str(result.inserted_id), record.get("id")),
                        )
                        updated_any = True

                    # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
                    cursor.execute(
                        f"UPDATE {table} SET sync_status = 'synced' WHERE id = ?",
                        (record.get("id"),),
                    )
                    updated_any = True
                if updated_any:
                    self.repo.sqlite_conn.commit()
            finally:
                cursor.close()

        except Exception as e:
            logger.debug("ØªØ¬Ø§Ù‡Ù„ Ø®Ø·Ø£ Ù…Ø²Ø§Ù…Ù†Ø© %s: %s", table, e)

    # ==========================================
    # Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ
    # ==========================================

    def start_auto_sync(self):
        """ğŸš€ Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©"""
        if not self._auto_sync_enabled:
            logger.info("â„¹ï¸ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© ØºÙŠØ± Ù…ÙØ¹Ù„Ø© Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
            return

        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©...")
        self._shutdown = False

        # ØªÙ†Ø¸ÙŠÙ Ø£ÙŠ Ù…Ø¤Ù‚ØªØ§Øª Ù‚Ø¯ÙŠÙ…Ø© Ù‚Ø¨Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ø¯Ø¡
        for timer_name in (
            "_auto_sync_timer",
            "_quick_sync_timer",
            "_connection_timer",
            "_cloud_pull_timer",
            "_delta_pull_timer",
        ):
            timer = getattr(self, timer_name, None)
            if timer:
                try:
                    timer.stop()
                except Exception:
                    pass
                setattr(self, timer_name, None)

        # 1. Ù…Ø¤Ù‚Øª ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„ (ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©)
        self._connection_timer = QTimer(self)
        self._connection_timer.timeout.connect(self._check_connection)
        self._connection_timer.start(self._connection_check_interval)

        # 2. Ù…Ø¤Ù‚Øª Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©:
        # Delta Sync ÙŠÙ†ÙØ° push+pull Ø¨Ø§Ù„ÙØ¹Ù„ØŒ Ù„Ø°Ø§ Ù†ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Delta Ø£Ø³Ø±Ø¹.
        quick_seconds = max(1, self._quick_sync_interval // 1000)
        if self._delta_sync_interval_seconds < quick_seconds:
            self._quick_sync_timer = None
            logger.info(
                "â„¹ï¸ ØªÙ… ØªØ¹Ø·ÙŠÙ„ Quick Push Ø§Ù„Ø¯ÙˆØ±ÙŠ Ù„Ø£Ù† Delta Sync Ø£Ø³Ø±Ø¹ (%sØ« < %sØ«)",
                self._delta_sync_interval_seconds,
                quick_seconds,
            )
        else:
            self._quick_sync_timer = QTimer(self)
            self._quick_sync_timer.timeout.connect(self._quick_push_changes)
            self._quick_sync_timer.start(self._quick_sync_interval)

        # 3. Ù…Ø¤Ù‚Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© (ÙƒÙ„ 5 Ø¯Ù‚Ø§Ø¦Ù‚)
        self._auto_sync_timer = QTimer(self)
        self._auto_sync_timer.timeout.connect(self._auto_full_sync)
        self._auto_sync_timer.start(self._auto_sync_interval)

        # Safety cloud pull: only when delta sync is relatively slow.
        # With fast delta (<= 10s), this timer causes redundant heavy full-sync load.
        if self._delta_sync_interval_seconds > 10:
            self._cloud_pull_timer = QTimer(self)
            self._cloud_pull_timer.timeout.connect(self._cloud_pull_changes)
            self._cloud_pull_timer.start(CLOUD_PULL_INTERVAL_MS)
        else:
            self._cloud_pull_timer = None
            logger.info(
                "â„¹ï¸ ØªÙ… ØªØ¹Ø·ÙŠÙ„ Cloud Pull Ø§Ù„Ø¯ÙˆØ±ÙŠ Ù„Ø£Ù† Delta Sync Ø³Ø±ÙŠØ¹ (%s Ø«Ø§Ù†ÙŠØ©)",
                self._delta_sync_interval_seconds,
            )

        # 4. Ù…Ø²Ø§Ù…Ù†Ø© Ø£ÙˆÙ„ÙŠØ© Ø¨Ø¹Ø¯ 5 Ø«ÙˆØ§Ù†ÙŠ
        QTimer.singleShot(5000, self._initial_sync)

        # 5. âš¡ NEW: Ø¨Ø¯Ø¡ Delta Sync Ø¯ÙˆØ±ÙŠ Ù„Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©
        self.start_delta_sync(interval_seconds=self._delta_sync_interval_seconds)

        logger.info("â° Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: ÙƒÙ„ %s Ø«Ø§Ù†ÙŠØ©", self._auto_sync_interval // 1000)
        if self._quick_sync_timer:
            logger.info("â° Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª: ÙƒÙ„ %s Ø«Ø§Ù†ÙŠØ©", self._quick_sync_interval // 1000)
        else:
            logger.info("â° Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª: Ù…ÙØ¹Ø·Ù‘Ù„ (Delta Sync ÙŠØºØ·ÙŠÙ‡)")
        logger.info("â° Delta Sync: ÙƒÙ„ %s Ø«Ø§Ù†ÙŠØ©", self._delta_sync_interval_seconds)

    def stop_auto_sync(self):
        """â¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©"""
        if self._shutdown and not any(
            (
                self._auto_sync_timer,
                self._quick_sync_timer,
                self._cloud_pull_timer,
                self._connection_timer,
                self._delta_pull_timer,
            )
        ):
            return

        logger.info("â¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©...")
        self._shutdown = True  # âš¡ ØªØ¹ÙŠÙŠÙ† Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ø¥ØºÙ„Ø§Ù‚

        # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¤Ù‚ØªØ§Øª Ø¨Ø£Ù…Ø§Ù†
        try:
            if self._auto_sync_timer:
                try:
                    self._auto_sync_timer.stop()
                except (RuntimeError, AttributeError):
                    pass
                self._auto_sync_timer = None
        except Exception:
            pass

        try:
            if self._quick_sync_timer:
                try:
                    self._quick_sync_timer.stop()
                except (RuntimeError, AttributeError):
                    pass
                self._quick_sync_timer = None
        except Exception:
            pass

        try:
            if self._cloud_pull_timer:
                try:
                    self._cloud_pull_timer.stop()
                except (RuntimeError, AttributeError):
                    pass
                self._cloud_pull_timer = None
        except Exception:
            pass

        try:
            if self._connection_timer:
                try:
                    self._connection_timer.stop()
                except (RuntimeError, AttributeError):
                    pass
                self._connection_timer = None
        except Exception:
            pass

        try:
            if self._delta_pull_timer:
                try:
                    self._delta_pull_timer.stop()
                except (RuntimeError, AttributeError):
                    pass
                self._delta_pull_timer = None
        except Exception:
            pass

        try:
            if self._delta_thread and self._delta_thread.is_alive():
                self._delta_thread_stop.set()
                self._delta_thread.join(timeout=0.5)
        except Exception:
            pass
        self._delta_thread = None

        deadline = time.monotonic() + 1.0
        while time.monotonic() < deadline:
            try:
                delta_busy = bool(self._delta_cycle_lock.locked())
            except Exception:
                delta_busy = False
            if not self._is_syncing and not delta_busy:
                break
            time.sleep(0.05)

        logger.info("âœ… ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©")

    def stop(self):
        self.stop_auto_sync()

    def _check_connection(self):
        """ğŸ”Œ ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ - Ù…Ø­Ø³Ù‘Ù†"""
        if self._shutdown:  # âš¡ ØªØ¬Ø§Ù‡Ù„ Ø¥Ø°Ø§ ØªÙ… Ø§Ù„Ø¥ØºÙ„Ø§Ù‚
            return

        try:
            # âš¡ ÙØ­Øµ Ø£Ù† MongoDB client Ù„Ø§ ÙŠØ²Ø§Ù„ Ù…ØªØ§Ø­Ø§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
            if self.repo is None or self.repo.mongo_client is None or self.repo.mongo_db is None:
                current_status = False
            else:
                try:
                    # Ù…Ø­Ø§ÙˆÙ„Ø© ping Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø§ØªØµØ§Ù„ ÙØ¹Ø§Ù„
                    self.repo.mongo_client.admin.command("ping")
                    current_status = True
                except Exception:
                    current_status = False

            # Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø§Ø±Ø© Ø¹Ù†Ø¯ ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø§Ù„Ø© ÙÙ‚Ø·
            if current_status != self._last_online_status:
                previous_status = self._last_online_status
                self._last_online_status = current_status
                try:
                    if not self._shutdown:
                        self.connection_changed.emit(current_status)
                except RuntimeError:
                    return  # Qt object deleted

                if current_status:
                    logger.info("ğŸŸ¢ ØªÙ… Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„")
                    # Ù„Ø§ Ù†Ø·Ù„Ù‚ Full Sync ÙÙˆØ±ÙŠ ÙÙŠ Ø£ÙˆÙ„ ØªØ´ØºÙŠÙ„ (None -> True)
                    # Ù„Ø£Ù† _initial_sync Ù…Ø¬Ø¯ÙˆÙ„ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¨Ø¹Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù….
                    if previous_status is False:
                        QTimer.singleShot(300, self._run_full_sync_async)
                else:
                    logger.warning("ğŸ”´ Ø§Ù†Ù‚Ø·Ø¹ Ø§Ù„Ø§ØªØµØ§Ù„ - Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ ÙˆØ¶Ø¹ Offline")
        except Exception:
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
            pass

    def _initial_sync(self):
        """ğŸš€ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„ - ØªÙØ§Ø¶Ù„ÙŠØ© Ù„Ù„Ø³Ø±Ø¹Ø©"""
        if self._shutdown:
            return

        if not self.is_online:
            logger.info("ğŸ“´ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ØªØµØ§Ù„ - Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©")
            return

        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©...")

        def sync_thread():
            if self._shutdown:
                return
            try:
                result = self.full_sync_from_cloud()
                if result.get("success"):
                    logger.info("âœ… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©: ØªÙ… ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ÙƒØ§Ù…Ù„")
                else:
                    logger.warning("âš ï¸ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù„Ù… ØªÙƒØªÙ…Ù„: %s", result.get("reason"))
            except Exception as e:
                logger.warning("âš ï¸ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©: %s", e)

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… QTimer Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† daemon thread
        threading.Thread(target=sync_thread, daemon=True).start()

    def _auto_full_sync(self):
        """ğŸ”„ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© - ØªÙØ§Ø¶Ù„ÙŠØ© Ù„Ù„Ø³Ø±Ø¹Ø©"""
        if self._shutdown or self._is_syncing or not self.is_online:
            return

        self._run_full_sync_async()

    def _quick_push_changes(self):
        """âš¡ Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¨Ø³Ø±Ø¹Ø©"""
        if self._shutdown or self._is_syncing or not self.is_online:
            return

        try:
            # âš¡ Ø¥Ù†Ø´Ø§Ø¡ cursor Ø¬Ø¯ÙŠØ¯ Ù„ØªØ¬Ù†Ø¨ Recursive cursor error
            cursor = self.repo.get_cursor()
            has_pending = False

            try:
                for table in self.TABLES:
                    try:
                        cursor.execute(
                            f"""
                            SELECT COUNT(*) FROM {table}
                            WHERE sync_status != 'synced' OR sync_status IS NULL
                        """
                        )
                        count = cursor.fetchone()[0]
                        if count > 0:
                            has_pending = True
                            break
                    except Exception:
                        # ÙØ´Ù„ ÙØ­Øµ Ø§Ù„Ø¹Ù†ØµØ±
                        pass
            finally:
                cursor.close()  # âš¡ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù€ cursor

            if has_pending:

                def push_thread():
                    if self._shutdown:
                        return
                    try:
                        with self._lock:
                            self._push_pending_changes()
                        logger.debug("âš¡ ØªÙ… Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©")
                    except Exception as e:
                        logger.error("âŒ ÙØ´Ù„ Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª: %s", e)

                threading.Thread(target=push_thread, daemon=True).start()

        except Exception as e:
            logger.debug("Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª: %s", e)

    def set_auto_sync_interval(self, minutes: int):
        """â° ØªØºÙŠÙŠØ± ÙØªØ±Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©"""
        self._auto_sync_interval = minutes * 60 * 1000
        if self._auto_sync_timer:
            self._auto_sync_timer.setInterval(self._auto_sync_interval)
        logger.info("â° ØªÙ… ØªØºÙŠÙŠØ± ÙØªØ±Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¥Ù„Ù‰ %s Ø¯Ù‚ÙŠÙ‚Ø©", minutes)

    @property
    def is_online(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ ÙØ­Øµ Ø­Ø§Ù„Ø© MongoDB client"""
        if self.repo is None:
            return False

        # âš¡ ÙØ­Øµ Ø£Ù† MongoDB client Ù…ØªØ§Ø­ ÙˆÙ„Ù… ÙŠÙØºÙ„Ù‚
        if self.repo.mongo_client is None or self.repo.mongo_db is None:
            return False

        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© ping Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø§ØªØµØ§Ù„ ÙØ¹Ø§Ù„
            self.repo.mongo_client.admin.command("ping")
            return True
        except Exception:
            return False

    def _wait_for_connection(self, timeout: int = 10) -> bool:
        """âš¡ Ø§Ù†ØªØ¸Ø§Ø± Ø§ØªØµØ§Ù„ MongoDB Ù…Ø¹ timeout"""
        import time

        waited = 0
        while not self.is_online and waited < timeout:
            time.sleep(0.5)
            waited += 0.5
        return self.is_online

    def _run_full_sync_async(self):
        if self._shutdown or self._is_syncing or not self.is_online:
            return

        def worker():
            if self._shutdown:
                return
            try:
                self.full_sync_from_cloud()
            except Exception as e:
                logger.debug("Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©: %s", e)

        threading.Thread(target=worker, daemon=True).start()

    def _cloud_pull_changes(self):
        if self._shutdown or not self.is_online:
            return
        if self._is_syncing:
            return
        # If delta sync is fast, avoid redundant forced full syncs.
        if self._delta_sync_interval_seconds <= 10:
            return
        if self._last_full_sync_at:
            if (datetime.now() - self._last_full_sync_at).total_seconds() < 30:
                return
        self._run_full_sync_async()

    def full_sync_from_cloud(self) -> dict[str, Any]:
        """
        Ù…Ø²Ø§Ù…Ù†Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø© - MongoDB Ù‡Ùˆ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„ÙˆØ­ÙŠØ¯
        ÙŠØ­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        """
        # âš¡ ÙØ­Øµ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ø£ÙˆÙ„Ø§Ù‹
        if self._shutdown:
            return {"success": False, "reason": "shutdown"}

        # âš¡ Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø£ÙˆÙ„Ø§Ù‹
        if not self._wait_for_connection(timeout=10):
            logger.warning("ØºÙŠØ± Ù…ØªØµÙ„ - Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©")
            return {"success": False, "reason": "offline"}

        if self._is_syncing:
            return {"success": False, "reason": "already_syncing"}

        # âš¡ ÙØ­Øµ ÙØ¹Ù„ÙŠ Ø£Ù† MongoDB client Ù„Ø§ ÙŠØ²Ø§Ù„ Ù…ØªØ§Ø­Ø§Ù‹
        if self.repo is None or self.repo.mongo_client is None or self.repo.mongo_db is None:
            return {"success": False, "reason": "no_mongo_client"}

        try:
            self.repo.mongo_client.admin.command("ping")
        except Exception:
            logger.debug("MongoDB client Ù…ØºÙ„Ù‚ - ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©")
            return {"success": False, "reason": "mongo_client_closed"}

        self._is_syncing = True
        self._last_full_sync_at = datetime.now()
        self.sync_started.emit()

        results = {"success": True, "tables": {}, "total_synced": 0, "total_deleted": 0}

        try:
            with self._lock:
                # 1. Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
                self._push_pending_changes()

                # 2. Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
                self._sync_users_from_cloud()

                # 3. Ù…Ø²Ø§Ù…Ù†Ø© ÙƒÙ„ Ø¬Ø¯ÙˆÙ„
                for table in self.TABLES:
                    try:
                        stats = self._sync_table_from_cloud(table)
                        results["tables"][table] = stats
                        results["total_synced"] += stats.get("synced", 0)
                        results["total_deleted"] += stats.get("deleted", 0)
                    except Exception as e:
                        logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© %s: %s", table, e)
                        results["tables"][table] = {"error": str(e)}

            logger.info("âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: %s Ø³Ø¬Ù„", results["total_synced"])
            self._update_sync_metrics(success=True, records_synced=results["total_synced"])
            self.sync_completed.emit(results)

            # âš¡ Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø£Ø±ØµØ¯Ø© Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù†Ù‚Ø¯ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
            try:
                from services.accounting_service import AccountingService

                # Ø¥Ø¨Ø·Ø§Ù„ Ø§Ù„Ù€ cache Ø£ÙˆÙ„Ø§Ù‹
                AccountingService._hierarchy_cache = None
                AccountingService._hierarchy_cache_time = 0
                logger.info("ğŸ“Š ØªÙ… Ø¥Ø¨Ø·Ø§Ù„ cache Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª - Ø³ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ø¹Ù†Ø¯ ÙØªØ­ ØªØ§Ø¨ Ø§Ù„Ù…Ø­Ø§Ø³Ø¨Ø©")
            except Exception as e:
                logger.warning("âš ï¸ ÙØ´Ù„ Ø¥Ø¨Ø·Ø§Ù„ cache Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª: %s", e)

            # âš¡ Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø§Ø±Ø§Øª ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
            try:
                from core.signals import app_signals

                app_signals.emit_data_changed("clients")
                app_signals.emit_data_changed("projects")
                app_signals.emit_data_changed("accounts")
                app_signals.emit_data_changed("payments")
                app_signals.emit_data_changed("expenses")
                logger.info("ğŸ“¢ ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø§Ø±Ø§Øª ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©")
            except Exception as e:
                logger.warning("âš ï¸ ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØªØ­Ø¯ÙŠØ«: %s", e)

        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: %s", e)
            results["success"] = False
            results["error"] = str(e)
            self._update_sync_metrics(success=False, records_synced=0)
            self.sync_error.emit(str(e))

        finally:
            self._is_syncing = False
            self._flush_realtime_pull_queue()

        return results

    def _sync_table_from_cloud(self, table_name: str) -> dict[str, int]:
        """
        Ù…Ø²Ø§Ù…Ù†Ø© Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù…Ø¹ Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        """
        stats = {"synced": 0, "inserted": 0, "updated": 0, "deleted": 0, "linked": 0}

        try:
            # âš¡ ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„ Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… MongoDB
            if self._shutdown:
                return stats

            if self.repo is None or not self.repo.online:
                return stats

            # âš¡ ÙØ­Øµ Ø£Ù† MongoDB client Ù„Ø§ ÙŠØ²Ø§Ù„ Ù…ØªØ§Ø­Ø§Ù‹
            if self.repo.mongo_db is None or self.repo.mongo_client is None:
                return stats

            # âš¡ ÙØ­Øµ ÙØ¹Ù„ÙŠ Ø£Ù† Ø§Ù„Ù€ client Ù„Ù… ÙŠÙØºÙ„Ù‚
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© ping Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø§ØªØµØ§Ù„ ÙØ¹Ø§Ù„
                self.repo.mongo_client.admin.command("ping")
            except Exception:
                logger.debug(
                    "ØªÙ… ØªØ®Ø·ÙŠ Ù…Ø²Ø§Ù…Ù†Ø© %s - MongoDB client Ù…ØºÙ„Ù‚ Ø£Ùˆ ØºÙŠØ± Ù…ØªØ§Ø­",
                    table_name,
                )
                return stats

            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
            try:
                cloud_data = list(self.repo.mongo_db[table_name].find())
            except Exception as mongo_err:
                error_msg = str(mongo_err)
                if (
                    "Cannot use MongoClient after close" in error_msg
                    or "InvalidOperation" in error_msg
                ):
                    logger.debug("ØªÙ… ØªØ®Ø·ÙŠ Ù…Ø²Ø§Ù…Ù†Ø© %s - MongoDB client Ù…ØºÙ„Ù‚", table_name)
                    return stats
                raise

            if not cloud_data:
                logger.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ %s", table_name)
                return stats

            # âš¡ Ø¥Ù†Ø´Ø§Ø¡ cursor Ø¬Ø¯ÙŠØ¯ Ù„ØªØ¬Ù†Ø¨ Recursive cursor error
            cursor = self.repo.get_cursor()
            conn = self.repo.sqlite_conn
            unique_field = self.UNIQUE_FIELDS.get(table_name, "name")

            try:
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„
                cursor.execute(f"PRAGMA table_info({table_name})")
                table_columns = {row[1] for row in cursor.fetchall()}

                # Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù€ mongo_ids Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
                cloud_mongo_ids = set()
                logo_clients = 0
                logo_payload_bytes = 0

                for i, cloud_item in enumerate(cloud_data):
                    self.sync_progress.emit(table_name, i + 1, len(cloud_data))

                    mongo_id = str(cloud_item["_id"])
                    cloud_mongo_ids.add(mongo_id)
                    unique_value = cloud_item.get(unique_field)

                    # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    item_data = self._prepare_cloud_data(cloud_item, table_name=table_name)
                    item_data["_mongo_id"] = mongo_id
                    item_data["sync_status"] = "synced"

                    if table_name == "clients":
                        raw_logo = cloud_item.get("logo_data")
                        has_logo = bool(cloud_item.get("has_logo", False) or raw_logo)
                        if has_logo:
                            logo_clients += 1
                            if isinstance(raw_logo, str):
                                logo_payload_bytes += len(raw_logo.encode("utf-8"))

                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠ
                    local_id = self._find_local_record(
                        cursor, table_name, mongo_id, unique_field, unique_value, table_columns
                    )

                    # ØªØµÙÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ„
                    filtered = {k: v for k, v in item_data.items() if k in table_columns}

                    if local_id:
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
                        self._update_record(cursor, table_name, local_id, filtered)
                        stats["updated"] += 1
                    else:
                        # Ø¥Ø¯Ø±Ø§Ø¬ Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯
                        self._insert_record(cursor, table_name, filtered)
                        stats["inserted"] += 1

                    stats["synced"] += 1

                # Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
                deleted = self._delete_orphan_records(cursor, table_name, cloud_mongo_ids)
                stats["deleted"] = deleted

                conn.commit()
                logger.info(
                    "âœ… %s: +%s ~%s -%s",
                    table_name,
                    stats["inserted"],
                    stats["updated"],
                    stats["deleted"],
                )
                if table_name == "clients" and logo_clients > 0:
                    if self._lazy_logo_enabled:
                        logger.info(
                            "ğŸ“· clients: %s Ø¹Ù…ÙŠÙ„ Ù„Ø¯ÙŠÙ‡ Ø´Ø¹Ø§Ø± (metadata synced - lazy mode)",
                            logo_clients,
                        )
                        if logo_payload_bytes > 0:
                            logger.debug(
                                "ğŸ“· clients: ØªÙ… ØªØ®Ø·ÙŠ ØªØ­Ù…ÙŠÙ„ payload Ø¨Ø­Ø¬Ù… ØªÙ‚Ø±ÙŠØ¨ÙŠ %s Ø£Ø«Ù†Ø§Ø¡ full sync",
                                self._format_bytes(logo_payload_bytes),
                            )
                    else:
                        logger.info(
                            "ğŸ“· clients: ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø´Ø¹Ø§Ø±Ø§Øª %s Ø¹Ù…ÙŠÙ„ (Ø­Ø¬Ù… ØªÙ‚Ø±ÙŠØ¨ÙŠ %s)",
                            logo_clients,
                            self._format_bytes(logo_payload_bytes),
                        )

            finally:
                # âš¡ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù€ cursor
                try:
                    cursor.close()
                except Exception:
                    pass

        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© %s: %s", table_name, e)
            # âš¡ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù€ cursor ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
            try:
                cursor.close()
            except Exception:
                pass

        return stats

    def _find_local_record(
        self,
        cursor,
        table_name: str,
        mongo_id: str,
        unique_field: str,
        unique_value: Any,
        table_columns: set,
    ) -> int | None:
        """
        Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¨Ø¹Ø¯Ø© Ø·Ø±Ù‚ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª

        âš¡ NEW: Force Overwrite Logic for Projects
        - If local record exists with same ID but different _mongo_id â†’ DELETE local, INSERT remote
        - Server data is the Single Source of Truth
        """
        try:
            # 1. Ø§Ù„Ø¨Ø­Ø« Ø¨Ù€ _mongo_id Ø£ÙˆÙ„Ø§Ù‹
            cursor.execute(f"SELECT id FROM {table_name} WHERE _mongo_id = ?", (mongo_id,))
            row = cursor.fetchone()
            if row:
                return row[0]

            # 2. Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„ÙØ±ÙŠØ¯ - ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ mongo_id
            if unique_value and unique_field in table_columns:
                cursor.execute(
                    f"SELECT id, _mongo_id FROM {table_name} WHERE {unique_field} = ?",
                    (unique_value,),
                )
                row = cursor.fetchone()
                if row:
                    local_id = row[0]
                    existing_mongo_id = row[1]

                    # âš¡ FORCE OVERWRITE LOGIC (Projects only)
                    if (
                        table_name == "projects"
                        and existing_mongo_id
                        and existing_mongo_id != mongo_id
                    ):
                        # ID collision detected: local record has different _mongo_id
                        # This means it's a different record, just unlucky collision
                        # DELETE local record to allow remote data to be inserted
                        logger.warning(
                            "ğŸ”¥ [FORCE OVERWRITE] Project ID collision detected: "
                            "local_id=%s, local_mongo_id=%s, remote_mongo_id=%s. "
                            "Deleting local record to prioritize server data.",
                            local_id,
                            existing_mongo_id,
                            mongo_id,
                        )
                        safe_print(
                            f"âš ï¸ [FORCE OVERWRITE] Ø­Ø°Ù Ù…Ø´Ø±ÙˆØ¹ Ù…Ø­Ù„ÙŠ (ID={local_id}) "
                            f"Ù„Ø¥ÙØ³Ø§Ø­ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙŠØ±ÙØ± (mongo_id={mongo_id})"
                        )

                        # Delete the local record
                        cursor.execute(f"DELETE FROM {table_name} WHERE id = ?", (local_id,))

                        # Return None to signal that a new record should be inserted
                        return None

                    # âš¡ Ø¥ØµÙ„Ø§Ø­: ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ mongo_id Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø®ØªÙ„Ù (Ù„Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£Ø®Ø±Ù‰)
                    if existing_mongo_id != mongo_id:
                        cursor.execute(
                            f"UPDATE {table_name} SET _mongo_id = ? WHERE id = ?",
                            (mongo_id, local_id),
                        )
                    return local_id
        except Exception as e:
            logger.debug("Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø¬Ù„: %s", e)

        return None

    def _delete_orphan_records(self, cursor, table_name: str, valid_mongo_ids: set) -> int:
        """
        Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        (Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„ØªÙŠ Ù„Ù‡Ø§ _mongo_id Ù„ÙƒÙ†Ù‡ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©)
        """
        if not valid_mongo_ids:
            return 0

        # Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„ØªÙŠ Ù„Ù‡Ø§ _mongo_id
        cursor.execute(f"SELECT id, _mongo_id FROM {table_name} WHERE _mongo_id IS NOT NULL")
        local_records = cursor.fetchall()

        deleted = 0
        for row in local_records:
            local_id = row[0]
            local_mongo_id = row[1]

            if local_mongo_id and local_mongo_id not in valid_mongo_ids:
                cursor.execute(f"DELETE FROM {table_name} WHERE id = ?", (local_id,))
                deleted += 1
                logger.debug("Ø­Ø°Ù Ø³Ø¬Ù„ ÙŠØªÙŠÙ…: %s/%s", table_name, local_id)

        return deleted

    def _prepare_cloud_data(self, data: dict, table_name: str | None = None) -> dict:
        """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù„Ù„Ø­ÙØ¸ Ù…Ø­Ù„ÙŠØ§Ù‹."""
        item = dict(data)
        item.pop("_id", None)
        item.pop("id", None)

        if table_name == "clients":
            raw_logo = data.get("logo_data")
            has_logo = bool(data.get("has_logo", False) or raw_logo)
            item["has_logo"] = 1 if has_logo else 0

            logo_last_synced = data.get("logo_last_synced") or data.get("last_modified")
            if hasattr(logo_last_synced, "isoformat"):
                item["logo_last_synced"] = logo_last_synced.isoformat()
            elif logo_last_synced is not None:
                item["logo_last_synced"] = str(logo_last_synced)
            else:
                item["logo_last_synced"] = None

            if self._lazy_logo_enabled:
                # Lazy mode: keep metadata only and avoid writing heavy blob in normal pulls/full sync.
                item.pop("logo_data", None)
                if not has_logo:
                    item["logo_data"] = None
                if raw_logo:
                    logger.debug(
                        "ğŸ“· [%s] lazy mode: skipped logo_data payload (%s chars)",
                        data.get("name", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"),
                        len(str(raw_logo)),
                    )
            elif raw_logo:
                item["logo_data"] = raw_logo
                logger.debug(
                    "ğŸ“· [%s] logo_data payload synced (%s chars)",
                    data.get("name", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"),
                    len(str(raw_logo)),
                )

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        date_fields = [
            "created_at",
            "last_modified",
            "date",
            "issue_date",
            "due_date",
            "expiry_date",
            "start_date",
            "end_date",
            "last_attempt",
            "expires_at",
            "last_login",
        ]
        for field in date_fields:
            if field in item and hasattr(item[field], "isoformat"):
                item[field] = item[field].isoformat()

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ÙˆØ§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø¥Ù„Ù‰ JSON
        json_fields = ["items", "lines", "data", "milestones"]
        for field in json_fields:
            if field in item and isinstance(item[field], list | dict):
                item[field] = json.dumps(item[field], ensure_ascii=False)

        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        now = datetime.now().isoformat()
        if not item.get("created_at"):
            item["created_at"] = now
        if not item.get("last_modified"):
            item["last_modified"] = now

        return item

    def _update_record(self, cursor, table_name: str, local_id: int, data: dict):
        """ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ù…Ø­Ù„ÙŠ"""
        if not data:
            return

        set_clause = ", ".join([f"{k}=?" for k in data.keys()])
        values = list(data.values()) + [local_id]
        cursor.execute(f"UPDATE {table_name} SET {set_clause} WHERE id=?", values)

    def _insert_record(self, cursor, table_name: str, data: dict):
        """Ø¥Ø¯Ø±Ø§Ø¬ Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª"""
        if not data:
            return

        # âš¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ© Ù„Ù„Ø¯ÙØ¹Ø§Øª - ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¨Ù€ (project_id + date + amount)
        if table_name == "payments":
            project_id = data.get("project_id")
            date = data.get("date", "")
            amount = data.get("amount", 0)
            date_short = str(date)[:10] if date else ""

            if project_id and amount:
                try:
                    cursor.execute(
                        """SELECT id FROM payments
                           WHERE project_id = ? AND amount = ? AND date LIKE ?""",
                        (project_id, amount, f"{date_short}%"),
                    )
                    existing = cursor.fetchone()
                    if existing:
                        # ØªØ­Ø¯ÙŠØ« Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¥Ø¯Ø±Ø§Ø¬
                        self._update_record(cursor, table_name, existing[0], data)
                        logger.debug("ØªÙ… ØªØ­Ø¯ÙŠØ« Ø¯ÙØ¹Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©: %s - %s", project_id, amount)
                        return
                except Exception:
                    pass

        columns = ", ".join(data.keys())
        placeholders = ", ".join(["?" for _ in data])

        try:
            cursor.execute(
                f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})", list(data.values())
            )
        except Exception as e:
            # ÙÙŠ Ø­Ø§Ù„Ø© UNIQUE constraint - Ù†Ø­Ø§ÙˆÙ„ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¥Ø¯Ø±Ø§Ø¬
            if "UNIQUE constraint" in str(e):
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙˆØªØ­Ø¯ÙŠØ«Ù‡
                unique_field = self.UNIQUE_FIELDS.get(table_name, "name")
                unique_value = data.get(unique_field)
                mongo_id = data.get("_mongo_id")

                if unique_value:
                    try:
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
                        cursor.execute(
                            f"SELECT id FROM {table_name} WHERE {unique_field} = ?", (unique_value,)
                        )
                        row = cursor.fetchone()
                        if row:
                            self._update_record(cursor, table_name, row[0], data)
                            logger.debug("ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙƒØ±Ø±: %s", unique_value)
                            return
                    except Exception:
                        pass

                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø­Ø« Ø¨Ù€ mongo_id
                if mongo_id:
                    try:
                        cursor.execute(
                            f"SELECT id FROM {table_name} WHERE _mongo_id = ?", (mongo_id,)
                        )
                        row = cursor.fetchone()
                        if row:
                            self._update_record(cursor, table_name, row[0], data)
                            return
                    except Exception:
                        pass

                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø®Ø·Ø£ Ø¥Ø°Ø§ ÙØ´Ù„ ÙƒÙ„ Ø´ÙŠØ¡
                logger.debug("ØªØ¬Ø§Ù‡Ù„ Ø³Ø¬Ù„ Ù…ÙƒØ±Ø± ÙÙŠ %s", table_name)
            else:
                raise

    def _push_pending_changes(self):
        """
        Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ù…Ø¹Ù„Ù‚Ø© Ù„Ù„Ø³Ø­Ø§Ø¨Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø³Ø­Ø¨
        """
        # âš¡ ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„Ø¥ØºÙ„Ø§Ù‚
        if self._shutdown:
            return

        if not self.is_online:
            return

        if self.repo is None or self.repo.mongo_db is None or self.repo.mongo_client is None:
            logger.debug("ØªÙ… ØªØ®Ø·ÙŠ Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª - MongoDB client ØºÙŠØ± Ù…ØªØ§Ø­")
            return

        logger.info("ğŸ“¤ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©...")

        for table in self.TABLES:
            try:
                self._push_table_changes(table)
            except Exception as e:
                logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ø±ÙØ¹ %s: %s", table, e)

    def _push_table_changes(self, table_name: str):
        """Ø±ÙØ¹ ØªØºÙŠÙŠØ±Ø§Øª Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯"""
        # âš¡ ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„ Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… MongoDB
        if self._shutdown:
            return

        if self.repo is None or not self.repo.online:
            return

        if self.repo.mongo_db is None or self.repo.mongo_client is None:
            logger.debug("ØªÙ… ØªØ®Ø·ÙŠ Ø±ÙØ¹ %s - MongoDB client ØºÙŠØ± Ù…ØªØ§Ø­", table_name)
            return

        # âš¡ Ø¥Ù†Ø´Ø§Ø¡ cursor Ø¬Ø¯ÙŠØ¯ Ù„ØªØ¬Ù†Ø¨ Recursive cursor error
        try:
            cursor = self.repo.get_cursor()
        except Exception as e:
            logger.debug("ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ cursor: %s", e)
            return

        conn = self.repo.sqlite_conn
        unique_field = self.UNIQUE_FIELDS.get(table_name, "name")

        try:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¬Ù„Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
            cursor.execute(
                f"""
                SELECT * FROM {table_name}
                WHERE sync_status != 'synced' OR sync_status IS NULL
            """
            )
            unsynced = cursor.fetchall()
        except Exception as e:
            logger.debug("ÙØ´Ù„ Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¬Ù„Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©: %s", e)
            cursor.close()
            return

        if not unsynced:
            cursor.close()
            return

        try:
            collection = self.repo.mongo_db[table_name]
        except Exception as e:
            if "Cannot use MongoClient after close" in str(e):
                logger.warning("âš ï¸ MongoDB client Ù…ØºÙ„Ù‚ - ØªØ®Ø·ÙŠ Ø±ÙØ¹ %s", table_name)
            cursor.close()
            return

        pushed = 0

        try:
            for row in unsynced:
                row_dict = dict(row)
                local_id = row_dict.get("id")
                mongo_id = row_dict.get("_mongo_id")
                unique_value = row_dict.get(unique_field)
                sync_status = row_dict.get("sync_status")

                if sync_status == "deleted":
                    try:
                        if mongo_id:
                            from bson import ObjectId

                            collection.delete_one({"_id": ObjectId(mongo_id)})
                        elif unique_value:
                            collection.delete_one({unique_field: unique_value})
                        cursor.execute(
                            f"DELETE FROM {table_name} WHERE id = ?",
                            (local_id,),
                        )
                        pushed += 1
                    except Exception as e:
                        logger.error("âŒ ÙØ´Ù„ Ø­Ø°Ù %s/%s: %s", table_name, local_id, e)
                    continue

                cloud_data = self._prepare_data_for_cloud(row_dict)

                try:
                    if mongo_id:
                        from bson import ObjectId

                        collection.update_one({"_id": ObjectId(mongo_id)}, {"$set": cloud_data})
                    else:
                        # âš¡ ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø±Ø§Ø¬ - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ© Ù„Ù„Ø¯ÙØ¹Ø§Øª
                        existing = None

                        if table_name == "payments":
                            # Ø§Ù„Ø¨Ø­Ø« Ø¨Ù€ (project_id + date + amount)
                            project_id = row_dict.get("project_id")
                            date = row_dict.get("date", "")
                            amount = row_dict.get("amount", 0)
                            date_short = str(date)[:10] if date else ""

                            if project_id and amount:
                                existing = collection.find_one(
                                    {
                                        "project_id": project_id,
                                        "amount": amount,
                                        "date": {"$regex": f"^{date_short}"},
                                    }
                                )
                        elif unique_value:
                            existing = collection.find_one({unique_field: unique_value})

                        if existing:
                            # Ø±Ø¨Ø· Ø¨Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
                            mongo_id = str(existing["_id"])
                            collection.update_one({"_id": existing["_id"]}, {"$set": cloud_data})
                        else:
                            # Ø¥Ø¯Ø±Ø§Ø¬ Ø¬Ø¯ÙŠØ¯
                            result = collection.insert_one(cloud_data)
                            mongo_id = str(result.inserted_id)

                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠ
                    cursor.execute(
                        f"UPDATE {table_name} SET _mongo_id = ?, sync_status = 'synced' WHERE id = ?",
                        (mongo_id, local_id),
                    )
                    pushed += 1

                except Exception as e:
                    # âš¡ ØªØ¬Ø§Ù‡Ù„ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªÙƒØ±Ø§Ø±
                    if "duplicate key" in str(e).lower() or "E11000" in str(e):
                        logger.debug("ØªØ¬Ø§Ù‡Ù„ Ø³Ø¬Ù„ Ù…ÙƒØ±Ø± ÙÙŠ %s: %s", table_name, e)
                        # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¹Ù„Ù‰ Ø£ÙŠ Ø­Ø§Ù„
                        cursor.execute(
                            f"UPDATE {table_name} SET sync_status = 'synced' WHERE id = ?",
                            (local_id,),
                        )
                    else:
                        logger.error("âŒ ÙØ´Ù„ Ø±ÙØ¹ %s/%s: %s", table_name, local_id, e)

            try:
                conn.commit()
            except Exception:
                pass

            if pushed > 0:
                logger.info("ğŸ“¤ %s: Ø±ÙØ¹ %s Ø³Ø¬Ù„", table_name, pushed)

        finally:
            # âš¡ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù€ cursor
            try:
                cursor.close()
            except Exception:
                pass

    def _prepare_data_for_cloud(self, data: dict) -> dict:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø±ÙØ¹ Ù„Ù„Ø³Ø­Ø§Ø¨Ø©"""
        clean = {k: v for k, v in data.items() if k not in ["id", "_mongo_id", "sync_status"]}

        # âš¡ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ logo_data
        # Ø¥Ø°Ø§ ÙƒØ§Ù† logo_data ÙØ§Ø±Øº Ùˆ logo_path ÙØ§Ø±Øº = Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­Ø°Ù Ø§Ù„ØµÙˆØ±Ø© ØµØ±Ø§Ø­Ø©
        # Ø¥Ø°Ø§ ÙƒØ§Ù† logo_data ÙØ§Ø±Øº Ùˆ logo_path Ù…ÙˆØ¬ÙˆØ¯ = Ù„Ø§ Ù†Ø±ÙŠØ¯ Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙˆÙ‚ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        logo_data_value = clean.get("logo_data", None)
        logo_path_value = clean.get("logo_path", None)

        if "logo_data" in clean:
            if logo_data_value:
                # ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø© - Ø±ÙØ¹Ù‡Ø§ Ù„Ù„Ø³Ø­Ø§Ø¨Ø©
                logger.debug("ğŸ“· Ø±ÙØ¹ logo_data (%s Ø­Ø±Ù) Ù„Ù„Ø³Ø­Ø§Ø¨Ø©", len(logo_data_value))
            elif not logo_path_value:
                # logo_data ÙØ§Ø±Øº Ùˆ logo_path ÙØ§Ø±Øº = Ø­Ø°Ù ØµØ±ÙŠØ­ Ù„Ù„ØµÙˆØ±Ø©
                clean["logo_data"] = ""  # Ø¥Ø±Ø³Ø§Ù„ Ù‚ÙŠÙ…Ø© ÙØ§Ø±ØºØ© ØµØ±ÙŠØ­Ø© Ù„Ù„Ø­Ø°Ù
                logger.debug("ğŸ—‘ï¸ Ø­Ø°Ù logo_data Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø© (Ø­Ø°Ù ØµØ±ÙŠØ­)")
            else:
                # logo_data ÙØ§Ø±Øº Ù„ÙƒÙ† logo_path Ù…ÙˆØ¬ÙˆØ¯ = Ù„Ø§ Ù†Ø±ÙŠØ¯ Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙˆÙ‚ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
                del clean["logo_data"]
                logger.debug("ğŸ“· ØªÙ… ØªØ¬Ø§Ù‡Ù„ logo_data Ø§Ù„ÙØ§Ø±Øº (Ù„Ù† ÙŠØªÙ… Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙˆÙ‚ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©)")

        if "logo_path" in clean and not clean["logo_path"]:
            # Ø¥Ø°Ø§ ÙƒØ§Ù† logo_path ÙØ§Ø±ØºØŒ Ù†Ø±Ø³Ù„ Ù‚ÙŠÙ…Ø© ÙØ§Ø±ØºØ© ØµØ±ÙŠØ­Ø©
            clean["logo_path"] = ""

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        for field in [
            "date",
            "issue_date",
            "due_date",
            "expiry_date",
            "start_date",
            "end_date",
        ]:
            if field in clean and clean[field]:
                try:
                    if isinstance(clean[field], str):
                        clean[field] = datetime.fromisoformat(clean[field].replace("Z", "+00:00"))
                except (ValueError, TypeError):
                    pass

        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø·ÙˆØ§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠØ© ÙƒÙ†Øµ ISO Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ù€ watermark.
        now_iso = datetime.now().isoformat()
        created_at = clean.get("created_at")
        if not created_at:
            clean["created_at"] = now_iso
        else:
            clean["created_at"] = self._to_iso_timestamp(created_at)

        last_modified = clean.get("last_modified")
        if not last_modified:
            clean["last_modified"] = now_iso
        else:
            clean["last_modified"] = self._to_iso_timestamp(last_modified)

        # ØªØ­ÙˆÙŠÙ„ JSON strings Ø¥Ù„Ù‰ objects
        for field in ["items", "lines", "data", "milestones"]:
            if field in clean and clean[field]:
                try:
                    if isinstance(clean[field], str):
                        clean[field] = json.loads(clean[field])
                except (json.JSONDecodeError, TypeError):
                    pass

        return clean

    def _sync_users_from_cloud(self):
        """Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡ (Ù…Ù† ÙˆØ¥Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©)"""
        try:
            # âš¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… cursor Ù…Ù†ÙØµÙ„ Ù„ØªØ¬Ù†Ø¨ Recursive cursor error
            cursor = self.repo.get_cursor()
            conn = self.repo.sqlite_conn

            try:
                # === 1. Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠÙŠÙ† Ø§Ù„Ø¬Ø¯Ø¯/Ø§Ù„Ù…Ø¹Ø¯Ù„ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø© ===
                logger.info("ğŸ“¤ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©...")
                cursor.execute(
                    """
                    SELECT * FROM users
                    WHERE sync_status IN ('new_offline', 'modified_offline', 'pending')
                       OR _mongo_id IS NULL
                """
                )
                local_pending = cursor.fetchall()

                uploaded_count = 0
                for row in local_pending:
                    user_data = dict(row)
                    username = user_data.get("username")
                    local_id = user_data.get("id")

                    existing_cloud = self.repo.mongo_db.users.find_one({"username": username})

                    if existing_cloud:
                        mongo_id = str(existing_cloud["_id"])
                        update_data = {
                            "full_name": user_data.get("full_name"),
                            "email": user_data.get("email"),
                            "role": user_data.get("role"),
                            "is_active": bool(user_data.get("is_active", 1)),
                            "last_modified": datetime.now(),
                        }
                        if user_data.get("password_hash"):
                            update_data["password_hash"] = user_data["password_hash"]

                        self.repo.mongo_db.users.update_one(
                            {"_id": existing_cloud["_id"]}, {"$set": update_data}
                        )
                        cursor.execute(
                            "UPDATE users SET _mongo_id=?, sync_status='synced' WHERE id=?",
                            (mongo_id, local_id),
                        )
                        uploaded_count += 1
                    else:
                        new_user = {
                            "username": username,
                            "password_hash": user_data.get("password_hash"),
                            "full_name": user_data.get("full_name"),
                            "email": user_data.get("email"),
                            "role": user_data.get("role", "sales"),
                            "is_active": bool(user_data.get("is_active", 1)),
                            "created_at": datetime.now(),
                            "last_modified": datetime.now(),
                        }
                        result = self.repo.mongo_db.users.insert_one(new_user)
                        mongo_id = str(result.inserted_id)
                        cursor.execute(
                            "UPDATE users SET _mongo_id=?, sync_status='synced' WHERE id=?",
                            (mongo_id, local_id),
                        )
                        uploaded_count += 1

                if uploaded_count > 0:
                    conn.commit()
                    logger.info("ğŸ“¤ ØªÙ… Ø±ÙØ¹ %s Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ø³Ø­Ø§Ø¨Ø©", uploaded_count)

                # === 2. ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø© ===
                logger.info("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©...")
                cloud_users = list(self.repo.mongo_db.users.find())
                if not cloud_users:
                    return

                downloaded_count = 0
                for u in cloud_users:
                    mongo_id = str(u["_id"])
                    username = u.get("username")

                    for field in ["created_at", "last_modified", "last_login"]:
                        if field in u and hasattr(u[field], "isoformat"):
                            u[field] = u[field].isoformat()

                    cursor.execute(
                        "SELECT id, sync_status FROM users WHERE _mongo_id = ? OR username = ?",
                        (mongo_id, username),
                    )
                    exists = cursor.fetchone()

                    if exists:
                        if exists[1] not in ("modified_offline", "new_offline"):
                            cursor.execute(
                                """
                                UPDATE users SET
                                    full_name=?, email=?, role=?, is_active=?,
                                    password_hash=?, _mongo_id=?, sync_status='synced',
                                    last_modified=?
                                WHERE id=?
                            """,
                                (
                                    u.get("full_name"),
                                    u.get("email"),
                                    u.get("role"),
                                    u.get("is_active", 1),
                                    u.get("password_hash"),
                                    mongo_id,
                                    u.get("last_modified", datetime.now().isoformat()),
                                    exists[0],
                                ),
                            )
                            downloaded_count += 1
                    else:
                        cursor.execute(
                            """
                            INSERT INTO users (
                                _mongo_id, username, full_name, email, role,
                                password_hash, is_active, sync_status, created_at, last_modified
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, 'synced', ?, ?)
                        """,
                            (
                                mongo_id,
                                username,
                                u.get("full_name"),
                                u.get("email"),
                                u.get("role"),
                                u.get("password_hash"),
                                u.get("is_active", 1),
                                u.get("created_at", datetime.now().isoformat()),
                                u.get("last_modified", datetime.now().isoformat()),
                            ),
                        )
                        downloaded_count += 1

                conn.commit()
                logger.info(
                    "âœ… ØªÙ… Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† (Ø±ÙØ¹: %sØŒ ØªÙ†Ø²ÙŠÙ„: %s)",
                    uploaded_count,
                    downloaded_count,
                )

            finally:
                cursor.close()

        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: %s", e)

    # ==========================================
    # Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    # ==========================================

    def remove_duplicates(self, table_name: str | None = None) -> dict[str, int]:
        """
        Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        ÙŠØ­ØªÙØ¸ Ø¨Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø£Ù‚Ø¯Ù… (Ø£Ù‚Ù„ id) ÙˆÙŠØ­Ø°Ù Ø§Ù„Ø¨Ø§Ù‚ÙŠ
        """
        tables = [table_name] if table_name else self.TABLES
        results = {}

        # âš¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… cursor Ù…Ù†ÙØµÙ„ Ù„ØªØ¬Ù†Ø¨ Recursive cursor error
        cursor = self.repo.get_cursor()
        conn = self.repo.sqlite_conn

        try:
            for table in tables:
                try:
                    unique_field = self.UNIQUE_FIELDS.get(table, "name")

                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
                    cursor.execute(
                        f"""
                        SELECT {unique_field}, COUNT(*) as cnt, MIN(id) as keep_id
                        FROM {table}
                        WHERE {unique_field} IS NOT NULL
                        GROUP BY {unique_field}
                        HAVING cnt > 1
                    """
                    )
                    duplicates = cursor.fetchall()

                    deleted = 0
                    for dup in duplicates:
                        unique_value = dup[0]
                        keep_id = dup[2]

                        # Ø­Ø°Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª (Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø£Ù‚Ø¯Ù…)
                        cursor.execute(
                            f"""
                            DELETE FROM {table}
                            WHERE {unique_field} = ? AND id != ?
                        """,
                            (unique_value, keep_id),
                        )
                        deleted += cursor.rowcount

                    conn.commit()
                    results[table] = deleted

                    if deleted > 0:
                        logger.info("ğŸ—‘ï¸ %s: Ø­Ø°Ù %s Ø³Ø¬Ù„ Ù…ÙƒØ±Ø±", table, deleted)

                except Exception as e:
                    logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø±Ø§Øª %s: %s", table, e)
                    results[table] = 0
        finally:
            cursor.close()

        return results

    def force_full_resync(self) -> dict[str, Any]:
        """
        Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø²Ø§Ù…Ù†Ø© ÙƒØ§Ù…Ù„Ø© Ù‚Ø³Ø±ÙŠØ©
        1. Ø­Ø°Ù ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        2. Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        """
        if not self.is_online:
            return {"success": False, "reason": "offline"}

        logger.warning("âš ï¸ Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø§Ù„Ù‚Ø³Ø±ÙŠØ©...")

        # âš¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… cursor Ù…Ù†ÙØµÙ„ Ù„ØªØ¬Ù†Ø¨ Recursive cursor error
        cursor = self.repo.get_cursor()
        conn = self.repo.sqlite_conn

        try:
            # Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© (Ù…Ø§ Ø¹Ø¯Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†)
            for table in self.TABLES:
                try:
                    cursor.execute(f"DELETE FROM {table}")
                    logger.info("ğŸ—‘ï¸ ØªÙ… Ù…Ø³Ø­ %s", table)
                except Exception as e:
                    logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø³Ø­ %s: %s", table, e)

            conn.commit()
        finally:
            cursor.close()

        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        return self.full_sync_from_cloud()

    def sync_now(self) -> dict[str, Any]:
        """
        Legacy-compatible manual sync API.
        Performs push then pull and returns a compact summary.
        """
        if self._shutdown:
            result = {"success": False, "reason": "shutdown", "pushed": 0, "pulled": 0, "errors": 0}
            self._update_sync_metrics(success=False, records_synced=0)
            return result

        if not self.is_online:
            result = {"success": False, "reason": "offline", "pushed": 0, "pulled": 0, "errors": 0}
            self._update_sync_metrics(success=False, records_synced=0)
            return result

        push_result = self.push_local_changes()
        pull_result = self.pull_remote_changes()

        pushed = int(push_result.get("pushed", 0))
        pulled = int(pull_result.get("pulled", 0))
        deleted = int(push_result.get("deleted", 0)) + int(pull_result.get("deleted", 0))
        errors = int(push_result.get("errors", 0)) + int(pull_result.get("errors", 0))
        success = bool(push_result.get("success")) and bool(pull_result.get("success"))

        result: dict[str, Any] = {
            "success": success,
            "pushed": pushed,
            "pulled": pulled,
            "deleted": deleted,
            "errors": errors,
        }
        if not success:
            result["reason"] = (
                pull_result.get("reason") or push_result.get("reason") or "sync_failed"
            )

        self._update_sync_metrics(success=success, records_synced=(pushed + pulled))
        return result

    def get_sync_metrics(self) -> dict[str, Any]:
        """Legacy-compatible metrics API for settings screens."""
        with self._sync_metrics_lock:
            return dict(self._sync_metrics)

    def get_sync_status(self) -> dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
        # âš¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… cursor Ù…Ù†ÙØµÙ„ Ù„ØªØ¬Ù†Ø¨ Recursive cursor error
        cursor = self.repo.get_cursor()
        status = {"is_online": self.is_online, "is_syncing": self._is_syncing, "tables": {}}

        try:
            for table in self.TABLES:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    total = cursor.fetchone()[0]

                    cursor.execute(
                        f"""
                        SELECT COUNT(*) FROM {table}
                        WHERE sync_status != 'synced' OR sync_status IS NULL
                    """
                    )
                    pending = cursor.fetchone()[0]

                    status["tables"][table] = {
                        "total": total,
                        "pending": pending,
                        "synced": total - pending,
                    }
                except Exception:
                    status["tables"][table] = {"total": 0, "pending": 0, "synced": 0}
        finally:
            cursor.close()

        return status

    def remove_cloud_duplicates(self) -> dict[str, int]:
        """
        Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† MongoDB
        ÙŠØ­ØªÙØ¸ Ø¨Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø£Ù‚Ø¯Ù… (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ created_at)
        """
        if not self.is_online:
            return {}

        results = {}
        logger.info("ğŸ§¹ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©...")

        for table in self.TABLES:
            try:
                deleted = self._remove_cloud_table_duplicates(table)
                results[table] = deleted
                if deleted > 0:
                    logger.info("ğŸ—‘ï¸ %s: Ø­Ø°Ù %s Ø³Ø¬Ù„ Ù…ÙƒØ±Ø± Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©", table, deleted)
            except Exception as e:
                logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ %s Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©: %s", table, e)
                results[table] = 0

        return results

    def _remove_cloud_table_duplicates(self, table_name: str) -> int:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯ ÙÙŠ MongoDB"""
        unique_field = self.UNIQUE_FIELDS.get(table_name, "name")
        collection = self.repo.mongo_db[table_name]

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… aggregation
        pipeline = [
            {
                "$group": {
                    "_id": f"${unique_field}",
                    "count": {"$sum": 1},
                    "docs": {"$push": {"_id": "$_id", "created_at": "$created_at"}},
                }
            },
            {"$match": {"count": {"$gt": 1}}},
        ]

        duplicates = list(collection.aggregate(pipeline))
        deleted = 0

        for dup in duplicates:
            docs = dup["docs"]
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ created_at (Ø§Ù„Ø£Ù‚Ø¯Ù… Ø£ÙˆÙ„Ø§Ù‹)
            docs.sort(key=lambda x: x.get("created_at") or datetime.min)

            # Ø­Ø°Ù ÙƒÙ„ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ù…Ø§ Ø¹Ø¯Ø§ Ø§Ù„Ø£ÙˆÙ„
            for doc in docs[1:]:
                collection.delete_one({"_id": doc["_id"]})
                deleted += 1

        return deleted

    def full_cleanup_and_sync(self) -> dict[str, Any]:
        """
        ØªÙ†Ø¸ÙŠÙ ÙƒØ§Ù…Ù„ ÙˆÙ…Ø²Ø§Ù…Ù†Ø©:
        1. ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† MongoDB
        2. ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        3. Ù…Ø²Ø§Ù…Ù†Ø© ÙƒØ§Ù…Ù„Ø©
        """
        results = {"cloud_cleanup": {}, "local_cleanup": {}, "sync": {}}

        if self.is_online:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
            logger.info("â˜ï¸ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©...")
            results["cloud_cleanup"] = self.remove_cloud_duplicates()

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­Ù„ÙŠ
        logger.info("ğŸ’¾ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©...")
        results["local_cleanup"] = self.remove_duplicates()

        # Ù…Ø²Ø§Ù…Ù†Ø©
        if self.is_online:
            logger.info("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©...")
            results["sync"] = self.full_sync_from_cloud()

        return results

    # ==========================================
    # âš¡ Bidirectional Delta Sync - NEW
    # ==========================================

    def _get_watermark_file_path(self) -> Path | None:
        """Resolve a stable watermark file path next to the local SQLite DB."""
        try:
            db_path = None

            if hasattr(self.repo, "LOCAL_DB_FILE"):
                candidate = getattr(self.repo, "LOCAL_DB_FILE", None)
                if isinstance(candidate, str) and candidate:
                    db_path = candidate

            if not db_path and hasattr(self.repo, "sqlite_conn") and self.repo.sqlite_conn:
                cursor = self.repo.sqlite_conn.cursor()
                try:
                    cursor.execute("PRAGMA database_list")
                    rows = cursor.fetchall()
                    for row in rows:
                        file_path = row[2] if len(row) > 2 else None
                        if file_path:
                            db_path = file_path
                            break
                finally:
                    cursor.close()

            if not db_path or db_path == ":memory:":
                return None

            return Path(db_path).resolve().parent / "sync_watermarks.json"
        except Exception as e:
            logger.debug("ÙØ´Ù„ ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ù…Ù„Ù watermark: %s", e)
            return None

    def _load_watermarks(self):
        """ØªØ­Ù…ÙŠÙ„ Watermarks Ù…Ù† Ù…Ù„Ù Ù…Ø­Ù„ÙŠ"""
        try:
            watermark_file = self._get_watermark_file_path()
            if watermark_file and watermark_file.exists():
                with open(watermark_file, encoding="utf-8") as f:
                    self._watermarks = json.load(f)
                logger.info("ğŸ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ Watermarks: %s Ø¬Ø¯Ø§ÙˆÙ„", len(self._watermarks))
                return
            self._watermarks = {}
        except Exception as e:
            logger.debug("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Watermarks: %s", e)
            self._watermarks = {}

    def _save_watermarks(self):
        """Ø­ÙØ¸ Watermarks Ø¥Ù„Ù‰ Ù…Ù„Ù Ù…Ø­Ù„ÙŠ"""
        try:
            watermark_file = self._get_watermark_file_path()
            if not watermark_file:
                return
            with open(watermark_file, "w", encoding="utf-8") as f:
                json.dump(self._watermarks, f, ensure_ascii=False, indent=2)
            logger.debug("ğŸ“ ØªÙ… Ø­ÙØ¸ Watermarks")
        except Exception as e:
            logger.debug("ÙØ´Ù„ Ø­ÙØ¸ Watermarks: %s", e)

    def push_local_changes(self) -> dict[str, Any]:
        """
        âš¡ Push all locally modified records to MongoDB
        Returns: dict with counts of pushed records and any errors
        """
        if self.repo is None or self.repo.sqlite_conn is None:
            return {"success": False, "reason": "sqlite_closed"}
        if not self.is_online:
            return {"success": False, "reason": "offline"}

        if self._shutdown:
            return {"success": False, "reason": "shutdown"}

        results = {"success": True, "pushed": 0, "deleted": 0, "errors": 0}
        changed_tables: set[str] = set()

        try:
            cursor = self.repo.get_cursor()

            for table in self.TABLES:
                try:
                    before_pushed = results["pushed"]
                    before_deleted = results["deleted"]
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙˆÙ„
                    cursor.execute(
                        "SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table,)
                    )
                    if not cursor.fetchone():
                        continue

                    # Ø¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
                    # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø¨Ø¹Ø¶ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ØªØ¶Ø¨Ø· sync_status ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† dirty_flag.
                    cursor.execute(
                        f"""
                        SELECT * FROM {table}
                        WHERE dirty_flag = 1
                           OR sync_status IS NULL
                           OR sync_status IN ('new_offline', 'modified_offline', 'pending', 'deleted')
                           OR _mongo_id IS NULL
                    """
                    )
                    dirty_records = cursor.fetchall()

                    if not dirty_records:
                        continue

                    columns = [desc[0] for desc in cursor.description]
                    collection = self.repo.mongo_db[table]

                    for row in dirty_records:
                        try:
                            record = dict(zip(columns, row, strict=False))
                            local_id = record.get("id")
                            mongo_id = record.get("_mongo_id")
                            sync_status = str(record.get("sync_status") or "").lower()
                            is_deleted = bool(record.get("is_deleted", 0))
                            unique_field = self.UNIQUE_FIELDS.get(table, "name")
                            unique_value = record.get(unique_field)

                            if is_deleted or sync_status == "deleted":
                                # Ø­Ø°Ù Ù…Ù†Ø·Ù‚ÙŠ ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù„Ø¶Ù…Ø§Ù† Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø­Ø°Ù Ø¹Ø¨Ø± Delta Sync
                                now_dt = datetime.now()
                                remote_error = False
                                remote_matched = False

                                if mongo_id:
                                    try:
                                        try:
                                            from bson import ObjectId

                                            result = collection.update_one(
                                                {"_id": ObjectId(mongo_id)},
                                                {
                                                    "$set": {
                                                        "is_deleted": True,
                                                        "sync_status": "deleted",
                                                        "last_modified": now_dt,
                                                    }
                                                },
                                            )
                                        except Exception:
                                            result = collection.update_one(
                                                {"_id": mongo_id},
                                                {
                                                    "$set": {
                                                        "is_deleted": True,
                                                        "sync_status": "deleted",
                                                        "last_modified": now_dt,
                                                    }
                                                },
                                            )
                                        remote_matched = bool(
                                            getattr(result, "matched_count", 0)
                                            or getattr(result, "modified_count", 0)
                                        )
                                    except Exception as del_err:
                                        remote_error = True
                                        logger.debug("ØªØ¹Ø°Ø± ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø­Ø°Ù ÙÙŠ MongoDB: %s", del_err)

                                if not remote_matched and unique_value:
                                    try:
                                        result = collection.update_one(
                                            {unique_field: unique_value},
                                            {
                                                "$set": {
                                                    "is_deleted": True,
                                                    "sync_status": "deleted",
                                                    "last_modified": now_dt,
                                                }
                                            },
                                        )
                                        remote_matched = bool(
                                            getattr(result, "matched_count", 0)
                                            or getattr(result, "modified_count", 0)
                                        )
                                    except Exception as del_err:
                                        remote_error = True
                                        logger.debug(
                                            "ØªØ¹Ø°Ø± ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø­Ø°Ù Ø¨Ø§Ù„Ù€ unique field: %s", del_err
                                        )

                                if remote_error:
                                    results["errors"] += 1
                                    continue

                                # Ø­Ø°Ù Ù…Ø­Ù„ÙŠØ§Ù‹ Ø¨Ø¹Ø¯ Ù†Ø¬Ø§Ø­ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø£Ùˆ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø³Ø¬Ù„ ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
                                cursor.execute(f"DELETE FROM {table} WHERE id = ?", (local_id,))
                                results["deleted"] += 1
                            else:
                                # Upsert Ø¥Ù„Ù‰ MongoDB
                                clean_record = {
                                    k: v
                                    for k, v in record.items()
                                    if k not in ["id", "sync_status", "dirty_flag", "is_deleted"]
                                }
                                clean_record["last_modified"] = datetime.now().isoformat()

                                if mongo_id:
                                    try:
                                        from bson import ObjectId

                                        collection.update_one(
                                            {"_id": ObjectId(mongo_id)},
                                            {"$set": clean_record},
                                            upsert=True,
                                        )
                                    except Exception:
                                        collection.update_one(
                                            {"_id": mongo_id},
                                            {"$set": clean_record},
                                            upsert=True,
                                        )
                                else:
                                    result = collection.insert_one(clean_record)
                                    mongo_id = str(result.inserted_id)
                                    cursor.execute(
                                        f"UPDATE {table} SET _mongo_id = ? WHERE id = ?",
                                        (mongo_id, local_id),
                                    )

                                # ØªØ­Ø¯ÙŠØ« dirty_flag Ùˆ sync_status
                                cursor.execute(
                                    f"""
                                    UPDATE {table}
                                    SET dirty_flag = 0, sync_status = 'synced'
                                    WHERE id = ?
                                """,
                                    (local_id,),
                                )
                                results["pushed"] += 1

                        except Exception as e:
                            logger.debug("Ø®Ø·Ø£ ÙÙŠ Ø±ÙØ¹ Ø³Ø¬Ù„ Ù…Ù† %s: %s", table, e)
                            results["errors"] += 1

                    self.repo.sqlite_conn.commit()

                except Exception as e:
                    logger.debug("Ø®Ø·Ø£ ÙÙŠ Ø±ÙØ¹ Ø¬Ø¯ÙˆÙ„ %s: %s", table, e)
                finally:
                    if results["pushed"] > before_pushed or results["deleted"] > before_deleted:
                        changed_tables.add(table)

            cursor.close()

            if results["pushed"] > 0 or results["deleted"] > 0:
                logger.info("â¬†ï¸ Delta Ø±ÙØ¹: %sØŒ Ø­Ø°Ù: %s", results["pushed"], results["deleted"])
                self._emit_sync_pings(changed_tables)

        except Exception as e:
            if self._shutdown and self._is_closed_sqlite_error(e):
                logger.debug("ØªØ¬Ø§Ù‡Ù„ push Ø¨Ø¹Ø¯ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚: %s", e)
                return {"success": False, "reason": "shutdown"}
            logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ push_local_changes: %s", e)
            results["success"] = False
            results["error"] = str(e)

        return results

    def pull_remote_changes(self) -> dict[str, Any]:
        """
        âš¡ Pull changes from MongoDB since last sync (watermark-based delta sync)
        Only pulls records where last_modified > watermark
        Returns: dict with counts of pulled/deleted records
        """
        if self.repo is None or self.repo.sqlite_conn is None:
            return {"success": False, "reason": "sqlite_closed"}
        if not self.is_online:
            return {"success": False, "reason": "offline"}

        if self._shutdown:
            return {"success": False, "reason": "shutdown"}

        if self._is_syncing:
            return {"success": False, "reason": "already_syncing"}

        results = {"success": True, "pulled": 0, "deleted": 0, "errors": 0}
        changed_tables: set[str] = set()

        try:
            cursor = self.repo.get_cursor()

            for table in self.TABLES:
                try:
                    before_pulled = results["pulled"]
                    before_deleted = results["deleted"]

                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Watermark Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø¯ÙˆÙ„
                    watermark = self._watermarks.get(table, "1970-01-01T00:00:00")
                    watermark_dt = self._parse_iso_datetime(watermark)
                    if watermark_dt and watermark_dt > datetime.now() + timedelta(seconds=30):
                        fallback_dt = datetime.now() - timedelta(minutes=5)
                        watermark = fallback_dt.isoformat()
                        self._watermarks[table] = watermark
                        self._save_watermarks()

                    # Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ù…Ù† MongoDB Ø§Ù„Ù…Ø­Ø¯Ù‘Ø«Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù€ watermark
                    collection = self.repo.mongo_db[table]
                    query = self._build_last_modified_query(watermark)
                    projection = None
                    if table == "clients" and self._lazy_logo_enabled:
                        projection = {"logo_data": 0}
                    if projection is None:
                        remote_records = list(collection.find(query))
                    else:
                        try:
                            remote_records = list(collection.find(query, projection))
                        except TypeError:
                            # ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Fakes Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
                            remote_records = list(collection.find(query))

                    if not remote_records:
                        continue

                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ù…Ø­Ù„ÙŠØ§Ù‹
                    cursor.execute(
                        "SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table,)
                    )
                    if not cursor.fetchone():
                        continue

                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„
                    cursor.execute(f"PRAGMA table_info({table})")
                    table_columns = {row[1] for row in cursor.fetchall()}

                    max_timestamp = watermark
                    logo_clients = 0

                    for remote in remote_records:
                        try:
                            mongo_id = str(remote["_id"])
                            is_deleted = remote.get("is_deleted", False)
                            last_modified_iso = self._to_iso_timestamp(
                                remote.get("last_modified", "")
                            )

                            # ØªØ­Ø¯ÙŠØ« max_timestamp
                            if last_modified_iso and last_modified_iso > max_timestamp:
                                max_timestamp = last_modified_iso

                            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠ
                            cursor.execute(
                                f"SELECT id FROM {table} WHERE _mongo_id = ?", (mongo_id,)
                            )
                            local_row = cursor.fetchone()

                            if is_deleted:
                                # Ø­Ø°Ù Ù…Ù† MongoDB -> Ø­Ø°Ù Ù…Ø­Ù„ÙŠØ§Ù‹
                                if local_row:
                                    cursor.execute(
                                        f"DELETE FROM {table} WHERE id = ?", (local_row[0],)
                                    )
                                    results["deleted"] += 1
                            else:
                                # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                                item_data = self._prepare_cloud_data(remote, table_name=table)
                                item_data["_mongo_id"] = mongo_id
                                item_data["sync_status"] = "synced"
                                item_data["dirty_flag"] = 0
                                item_data["is_deleted"] = 0
                                if table == "clients" and bool(item_data.get("has_logo", 0)):
                                    logo_clients += 1

                                # ØªØµÙÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ„
                                filtered = {
                                    k: v for k, v in item_data.items() if k in table_columns
                                }

                                if local_row:
                                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
                                    set_clause = ", ".join([f"{k} = ?" for k in filtered.keys()])
                                    values = list(filtered.values()) + [local_row[0]]
                                    cursor.execute(
                                        f"UPDATE {table} SET {set_clause} WHERE id = ?", values
                                    )
                                else:
                                    # Ø¥Ø¯Ø±Ø§Ø¬ Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯
                                    cols = ", ".join(filtered.keys())
                                    placeholders = ", ".join(["?" for _ in filtered])
                                    cursor.execute(
                                        f"INSERT INTO {table} ({cols}) VALUES ({placeholders})",
                                        list(filtered.values()),
                                    )
                                results["pulled"] += 1

                        except Exception as e:
                            logger.debug("Ø®Ø·Ø£ ÙÙŠ Ø³Ø­Ø¨ Ø³Ø¬Ù„ Ù…Ù† %s: %s", table, e)
                            results["errors"] += 1

                    if remote_records:
                        # âš¡ CRITICAL: Update watermark based on the LATEST record found
                        try:
                            latest_ts = max(
                                (
                                    self._to_iso_timestamp(r.get("last_modified", ""))
                                    for r in remote_records
                                ),
                                default="",
                            )
                            current_watermark = self._watermarks.get(table, "")

                            if latest_ts and latest_ts > current_watermark:
                                self._watermarks[table] = latest_ts
                                self._save_watermarks()  # âš¡ Save immediately
                                logger.debug("ğŸ“ Watermark updated for %s: %s", table, latest_ts)
                        except Exception as wm_err:
                            logger.error("âŒ Failed to update watermark for %s: %s", table, wm_err)

                    self.repo.sqlite_conn.commit()
                    if table == "clients" and logo_clients > 0 and self._lazy_logo_enabled:
                        logger.info(
                            "ğŸ“· clients delta: %s Ø¹Ù…ÙŠÙ„ Ù„Ø¯ÙŠÙ‡ Ø´Ø¹Ø§Ø± (metadata only - lazy mode)",
                            logo_clients,
                        )
                    if results["pulled"] > before_pulled or results["deleted"] > before_deleted:
                        changed_tables.add(table)

                except Exception as e:
                    logger.debug("Ø®Ø·Ø£ ÙÙŠ Ø³Ø­Ø¨ Ø¬Ø¯ÙˆÙ„ %s: %s", table, e)

            cursor.close()

            # Ø­ÙØ¸ Ø§Ù„Ù€ watermarks
            self._save_watermarks()

            if results["pulled"] > 0 or results["deleted"] > 0:
                logger.info("â¬‡ï¸ Delta Ø³Ø­Ø¨: %sØŒ Ø­Ø°Ù: %s", results["pulled"], results["deleted"])
                if changed_tables:
                    try:
                        from core.signals import app_signals

                        for table_name in sorted(changed_tables):
                            app_signals.emit_ui_data_changed(table_name)
                    except Exception as signal_err:
                        logger.debug("ØªØ¹Ø°Ø± Ø¨Ø« Ø¥Ø´Ø§Ø±Ø§Øª UI Ø¨Ø¹Ø¯ delta pull: %s", signal_err)
                # Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø§Ø±Ø© Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
                try:
                    self.data_synced.emit()
                except RuntimeError:
                    pass  # Qt object deleted

        except Exception as e:
            if self._shutdown and self._is_closed_sqlite_error(e):
                logger.debug("ØªØ¬Ø§Ù‡Ù„ pull Ø¨Ø¹Ø¯ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚: %s", e)
                return {"success": False, "reason": "shutdown"}
            logger.error("âŒ Ø®Ø·Ø£ ÙÙŠ pull_remote_changes: %s", e)
            results["success"] = False
            results["error"] = str(e)

        return results

    def _run_delta_cycle(self) -> dict[str, Any]:
        """Run one guarded push+pull cycle without overlapping with another delta cycle."""
        if self.repo is None or self.repo.sqlite_conn is None:
            return {"success": False, "reason": "sqlite_closed"}
        if self._shutdown:
            return {"success": False, "reason": "shutdown"}
        if self._is_syncing:
            return {"success": False, "reason": "full_sync_in_progress"}

        if not self._delta_cycle_lock.acquire(blocking=False):
            return {"success": False, "reason": "delta_busy"}

        try:
            push_result = self.push_local_changes()
            pull_result = self.pull_remote_changes()
            return {
                "success": bool(push_result.get("success")) and bool(pull_result.get("success")),
                "pushed": int(push_result.get("pushed", 0)),
                "pulled": int(pull_result.get("pulled", 0)),
                "deleted": int(push_result.get("deleted", 0)) + int(pull_result.get("deleted", 0)),
                "errors": int(push_result.get("errors", 0)) + int(pull_result.get("errors", 0)),
            }
        finally:
            self._delta_cycle_lock.release()

    def _run_table_reconcile_cycle(self, table: str) -> dict[str, Any]:
        """
        Run one guarded targeted cycle for a single table.
        Used by remote notification-triggered pulls to bypass watermark stalls safely.
        """
        if table not in self.TABLES:
            return {"success": False, "reason": "invalid_table"}
        if self.repo is None or self.repo.sqlite_conn is None:
            return {"success": False, "reason": "sqlite_closed"}
        if self._shutdown:
            return {"success": False, "reason": "shutdown"}
        if self._is_syncing:
            return {"success": False, "reason": "full_sync_in_progress"}

        if not self._delta_cycle_lock.acquire(blocking=False):
            return {"success": False, "reason": "delta_busy"}

        pushed = 0
        errors = 0
        try:
            push_result = self.push_local_changes()
            pushed = int(push_result.get("pushed", 0))
            errors += int(push_result.get("errors", 0))
            self._sync_single_table_from_cloud(table)
            try:
                from core.signals import app_signals

                app_signals.emit_ui_data_changed(table)
            except Exception:
                pass
            try:
                self.data_synced.emit()
            except RuntimeError:
                pass
            return {
                "success": bool(push_result.get("success", True)),
                "pushed": pushed,
                "pulled": 0,
                "deleted": int(push_result.get("deleted", 0)),
                "errors": errors,
                "table": table,
            }
        except Exception as e:
            logger.debug("Ø®Ø·Ø£ ÙÙŠ table reconcile cycle (%s): %s", table, e)
            return {
                "success": False,
                "reason": "table_reconcile_failed",
                "table": table,
                "error": str(e),
            }
        finally:
            self._delta_cycle_lock.release()

    def force_pull(self, table: str = None):
        """
        âš¡ Force immediate pull (for screen open events)
        Pushes local changes first, then pulls remote changes
        """
        if self._shutdown or self._is_syncing or not self.is_online:
            return

        def pull_thread():
            if self._shutdown:
                return
            try:
                target_table = table if isinstance(table, str) and table in self.TABLES else None
                if target_table:
                    self._run_table_reconcile_cycle(target_table)
                else:
                    # Fallback: full delta cycle for broad refresh.
                    self._run_delta_cycle()
            except Exception as e:
                logger.debug("Ø®Ø·Ø£ ÙÙŠ force_pull: %s", e)

        threading.Thread(target=pull_thread, daemon=True).start()

    def start_delta_sync(self, interval_seconds: int = DEFAULT_DELTA_SYNC_INTERVAL_SECONDS):
        """
        âš¡ Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙØ§Ø¶Ù„ÙŠØ© (Delta Sync)
        ÙŠÙ‚ÙˆÙ… Ø¨Ø³Ø­Ø¨ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙƒÙ„ ÙØªØ±Ø© Ù…Ø­Ø¯Ø¯Ø©
        """
        if self._delta_pull_timer:
            try:
                self._delta_pull_timer.stop()
            except Exception:
                pass
            self._delta_pull_timer = None

        if self._delta_thread and self._delta_thread.is_alive():
            try:
                self._delta_thread_stop.set()
                self._delta_thread.join(timeout=0.5)
            except Exception:
                pass
        self._delta_thread_stop = threading.Event()

        interval_seconds = self._safe_int(
            interval_seconds, DEFAULT_DELTA_SYNC_INTERVAL_SECONDS, 1, 300
        )
        self._delta_sync_interval_seconds = interval_seconds
        interval_seconds = max(1, int(interval_seconds))

        def delta_loop():
            next_run = time.monotonic()
            while not self._shutdown and not self._delta_thread_stop.is_set():
                next_run += interval_seconds
                sleep_for = max(0.0, next_run - time.monotonic())
                if sleep_for:
                    time.sleep(sleep_for)
                if self._shutdown or self._delta_thread_stop.is_set():
                    break
                if self._is_syncing or not self.is_online:
                    continue
                try:
                    self._run_delta_cycle()
                except Exception as e:
                    logger.debug("Ø®Ø·Ø£ ÙÙŠ periodic delta sync: %s", e)

        self._delta_thread = threading.Thread(
            target=delta_loop,
            daemon=True,
            name="unified-delta-sync",
        )
        self._delta_thread.start()

        logger.info("â° Ø¨Ø¯Ø¡ Delta Sync ÙƒÙ„ %s Ø«Ø§Ù†ÙŠØ©", interval_seconds)


def create_unified_sync_manager(repository) -> UnifiedSyncManagerV3:
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯ÙŠØ± Ù…Ø²Ø§Ù…Ù†Ø© Ù…ÙˆØ­Ø¯"""
    return UnifiedSyncManagerV3(repository)
