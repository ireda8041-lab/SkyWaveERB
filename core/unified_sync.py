# Ø§Ù„Ù…Ù„Ù: core/unified_sync.py
"""
ğŸ”„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…ÙˆØ­Ø¯ - MongoDB First
MongoDB Ù‡Ùˆ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØŒ SQLite Ù†Ø³Ø®Ø© Ù…Ø­Ù„ÙŠØ© Ù„Ù„Ù€ offline ÙÙ‚Ø·

Ø§Ù„Ù…Ø¨Ø¯Ø£:
- Ø¹Ù†Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„: MongoDB = Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©
- Ø¹Ù†Ø¯ Ø¹Ø¯Ù… Ø§Ù„Ø§ØªØµØ§Ù„: SQLite ÙŠØ­ÙØ¸ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ù…Ø¤Ù‚ØªØ§Ù‹
- Ø¹Ù†Ø¯ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„: Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø«Ù… Ù…Ø³Ø­ ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ù…Ù† MongoDB
"""

import json
import threading
from datetime import datetime
from typing import Any

from PyQt6.QtCore import QObject, pyqtSignal

from core.logger import get_logger

logger = get_logger(__name__)


class UnifiedSyncManagerV3(QObject):
    """
    Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…ÙˆØ­Ø¯ - MongoDB First Architecture
    """

    # Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª
    sync_started = pyqtSignal()
    sync_progress = pyqtSignal(str, int, int)  # table, current, total
    sync_completed = pyqtSignal(dict)
    sync_error = pyqtSignal(str)

    # Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
    TABLES = [
        'accounts', 'clients', 'services', 'projects',
        'invoices', 'payments', 'expenses', 'journal_entries',
        'quotations', 'currencies', 'notifications', 'tasks'
    ]

    # Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù„ÙƒÙ„ Ø¬Ø¯ÙˆÙ„
    UNIQUE_FIELDS = {
        'clients': 'name',
        'projects': 'name',
        'services': 'name',
        'accounts': 'code',
        'invoices': 'invoice_number',
        'payments': 'id',
        'expenses': 'id',
        'journal_entries': 'id',
        'quotations': 'quote_number',
        'currencies': 'code',
        'users': 'username',
        'notifications': 'id',
        'tasks': 'id'
    }

    def __init__(self, repository, parent=None):
        super().__init__(parent)
        self.repo = repository
        self._lock = threading.RLock()
        self._is_syncing = False

        logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© UnifiedSyncManager - MongoDB First")

    @property
    def is_online(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„"""
        return self.repo.online if self.repo else False

    def full_sync_from_cloud(self) -> dict[str, Any]:
        """
        Ù…Ø²Ø§Ù…Ù†Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø© - MongoDB Ù‡Ùˆ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„ÙˆØ­ÙŠØ¯
        ÙŠØ­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        """
        if not self.is_online:
            logger.warning("ØºÙŠØ± Ù…ØªØµÙ„ - Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©")
            return {'success': False, 'reason': 'offline'}

        if self._is_syncing:
            return {'success': False, 'reason': 'already_syncing'}

        self._is_syncing = True
        self.sync_started.emit()

        results = {
            'success': True,
            'tables': {},
            'total_synced': 0,
            'total_deleted': 0
        }

        try:
            with self._lock:
                # 1. Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
                self._push_pending_changes()

                # 2. Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
                self._sync_users_from_cloud()

                # 3. Ù…Ø²Ø§Ù…Ù†Ø© ÙƒÙ„ Ø¬Ø¯ÙˆÙ„
                for table in self.TABLES:
                    try:
                        stats = self._sync_table_from_cloud(table)
                        results['tables'][table] = stats
                        results['total_synced'] += stats.get('synced', 0)
                        results['total_deleted'] += stats.get('deleted', 0)
                    except Exception as e:
                        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© {table}: {e}")
                        results['tables'][table] = {'error': str(e)}

            logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {results['total_synced']} Ø³Ø¬Ù„")
            self.sync_completed.emit(results)
            
            # âš¡ Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø§Ø±Ø§Øª ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
            try:
                from core.signals import app_signals
                app_signals.emit_data_changed('clients')
                app_signals.emit_data_changed('projects')
                logger.info("ğŸ“¢ ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø§Ø±Ø§Øª ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©")
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØªØ­Ø¯ÙŠØ«: {e}")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: {e}")
            results['success'] = False
            results['error'] = str(e)
            self.sync_error.emit(str(e))

        finally:
            self._is_syncing = False

        return results

    def _sync_table_from_cloud(self, table_name: str) -> dict[str, int]:
        """
        Ù…Ø²Ø§Ù…Ù†Ø© Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù…Ø¹ Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        """
        stats = {'synced': 0, 'inserted': 0, 'updated': 0, 'deleted': 0, 'linked': 0}

        try:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
            cloud_data = list(self.repo.mongo_db[table_name].find())
            
            if not cloud_data:
                logger.info(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ {table_name}")
                return stats

            cursor = self.repo.sqlite_cursor
            conn = self.repo.sqlite_conn
            unique_field = self.UNIQUE_FIELDS.get(table_name, 'name')

            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„
            cursor.execute(f"PRAGMA table_info({table_name})")
            table_columns = {row[1] for row in cursor.fetchall()}

            # Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù€ mongo_ids Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
            cloud_mongo_ids = set()

            for i, cloud_item in enumerate(cloud_data):
                self.sync_progress.emit(table_name, i + 1, len(cloud_data))

                mongo_id = str(cloud_item['_id'])
                cloud_mongo_ids.add(mongo_id)
                unique_value = cloud_item.get(unique_field)

                # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                item_data = self._prepare_cloud_data(cloud_item)
                item_data['_mongo_id'] = mongo_id
                item_data['sync_status'] = 'synced'

                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠ
                local_id = self._find_local_record(
                    cursor, table_name, mongo_id, unique_field, unique_value, table_columns
                )

                # ØªØµÙÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ„
                filtered = {k: v for k, v in item_data.items() if k in table_columns}
                
                # âš¡ ØªØ³Ø¬ÙŠÙ„ Ù„Ùˆ logo_data Ù…ÙˆØ¬ÙˆØ¯
                if table_name == 'clients' and 'logo_data' in item_data and item_data['logo_data']:
                    if 'logo_data' in filtered:
                        logger.info(f"ğŸ“· [{unique_value}] logo_data Ø³ÙŠØªÙ… Ø­ÙØ¸Ù‡ ({len(filtered['logo_data'])} Ø­Ø±Ù)")
                    else:
                        logger.warning(f"âš ï¸ [{unique_value}] logo_data ØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡! (ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„)")
                        logger.warning(f"   Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„: {table_columns}")

                if local_id:
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
                    self._update_record(cursor, table_name, local_id, filtered)
                    stats['updated'] += 1
                else:
                    # Ø¥Ø¯Ø±Ø§Ø¬ Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯
                    self._insert_record(cursor, table_name, filtered)
                    stats['inserted'] += 1

                stats['synced'] += 1

            # Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
            deleted = self._delete_orphan_records(cursor, table_name, cloud_mongo_ids)
            stats['deleted'] = deleted

            conn.commit()
            logger.info(f"âœ… {table_name}: +{stats['inserted']} ~{stats['updated']} -{stats['deleted']}")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© {table_name}: {e}")
            import traceback
            traceback.print_exc()

        return stats

    def _find_local_record(
        self, cursor, table_name: str, mongo_id: str,
        unique_field: str, unique_value: Any, table_columns: set
    ) -> int | None:
        """
        Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¨Ø¹Ø¯Ø© Ø·Ø±Ù‚ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        """
        # 1. Ø§Ù„Ø¨Ø­Ø« Ø¨Ù€ _mongo_id
        cursor.execute(
            f"SELECT id FROM {table_name} WHERE _mongo_id = ?",
            (mongo_id,)
        )
        row = cursor.fetchone()
        if row:
            return row[0]

        # 2. Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„ÙØ±ÙŠØ¯
        if unique_value and unique_field in table_columns:
            cursor.execute(
                f"SELECT id, _mongo_id FROM {table_name} WHERE {unique_field} = ?",
                (unique_value,)
            )
            row = cursor.fetchone()
            if row:
                local_id = row[0]
                existing_mongo_id = row[1]

                # Ø¥Ø°Ø§ Ø§Ù„Ø³Ø¬Ù„ ØºÙŠØ± Ù…Ø±ØªØ¨Ø· Ø£Ùˆ Ù…Ø±ØªØ¨Ø· Ø¨Ù†ÙØ³ Ø§Ù„Ù€ mongo_id
                if not existing_mongo_id or existing_mongo_id == mongo_id:
                    return local_id

        return None

    def _delete_orphan_records(
        self, cursor, table_name: str, valid_mongo_ids: set
    ) -> int:
        """
        Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        (Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„ØªÙŠ Ù„Ù‡Ø§ _mongo_id Ù„ÙƒÙ†Ù‡ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©)
        """
        if not valid_mongo_ids:
            return 0

        # Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„ØªÙŠ Ù„Ù‡Ø§ _mongo_id
        cursor.execute(
            f"SELECT id, _mongo_id FROM {table_name} WHERE _mongo_id IS NOT NULL"
        )
        local_records = cursor.fetchall()

        deleted = 0
        for row in local_records:
            local_id = row[0]
            local_mongo_id = row[1]

            if local_mongo_id and local_mongo_id not in valid_mongo_ids:
                cursor.execute(f"DELETE FROM {table_name} WHERE id = ?", (local_id,))
                deleted += 1
                logger.debug(f"Ø­Ø°Ù Ø³Ø¬Ù„ ÙŠØªÙŠÙ…: {table_name}/{local_id}")

        return deleted

    def _prepare_cloud_data(self, data: dict) -> dict:
        """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù„Ù„Ø­ÙØ¸ Ù…Ø­Ù„ÙŠØ§Ù‹"""
        item = dict(data)
        item.pop('_id', None)
        item.pop('id', None)

        # âš¡ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¬Ù„Ø¨ logo_data Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
        if 'logo_data' in data and data['logo_data']:
            item['logo_data'] = data['logo_data']
            client_name = data.get('name', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')
            logger.info(f"ğŸ“· [{client_name}] Ø¬Ù„Ø¨ logo_data ({len(data['logo_data'])} Ø­Ø±Ù) Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©")
            print(f"INFO: ğŸ“· [{client_name}] Ø¬Ù„Ø¨ logo_data ({len(data['logo_data'])} Ø­Ø±Ù) Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©")

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        date_fields = [
            'created_at', 'last_modified', 'date', 'issue_date',
            'due_date', 'expiry_date', 'start_date', 'end_date',
            'last_attempt', 'expires_at', 'last_login'
        ]
        for field in date_fields:
            if field in item and hasattr(item[field], 'isoformat'):
                item[field] = item[field].isoformat()

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ÙˆØ§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø¥Ù„Ù‰ JSON
        json_fields = ['items', 'lines', 'data', 'milestones']
        for field in json_fields:
            if field in item and isinstance(item[field], (list, dict)):
                item[field] = json.dumps(item[field], ensure_ascii=False)

        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        now = datetime.now().isoformat()
        if not item.get('created_at'):
            item['created_at'] = now
        if not item.get('last_modified'):
            item['last_modified'] = now

        return item

    def _update_record(self, cursor, table_name: str, local_id: int, data: dict):
        """ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ù…Ø­Ù„ÙŠ"""
        if not data:
            return

        set_clause = ', '.join([f"{k}=?" for k in data.keys()])
        values = list(data.values()) + [local_id]
        cursor.execute(
            f"UPDATE {table_name} SET {set_clause} WHERE id=?",
            values
        )

    def _insert_record(self, cursor, table_name: str, data: dict):
        """Ø¥Ø¯Ø±Ø§Ø¬ Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯"""
        if not data:
            return

        columns = ', '.join(data.keys())
        placeholders = ', '.join(['?' for _ in data])
        cursor.execute(
            f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})",
            list(data.values())
        )


    def _push_pending_changes(self):
        """
        Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ù…Ø¹Ù„Ù‚Ø© Ù„Ù„Ø³Ø­Ø§Ø¨Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø³Ø­Ø¨
        """
        if not self.is_online:
            return

        logger.info("ğŸ“¤ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©...")

        for table in self.TABLES:
            try:
                self._push_table_changes(table)
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø±ÙØ¹ {table}: {e}")

    def _push_table_changes(self, table_name: str):
        """Ø±ÙØ¹ ØªØºÙŠÙŠØ±Ø§Øª Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯"""
        cursor = self.repo.sqlite_cursor
        conn = self.repo.sqlite_conn
        unique_field = self.UNIQUE_FIELDS.get(table_name, 'name')

        # Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¬Ù„Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
        cursor.execute(f"""
            SELECT * FROM {table_name}
            WHERE sync_status != 'synced' OR sync_status IS NULL
        """)
        unsynced = cursor.fetchall()

        if not unsynced:
            return

        collection = self.repo.mongo_db[table_name]
        pushed = 0

        for row in unsynced:
            row_dict = dict(row)
            local_id = row_dict.get('id')
            mongo_id = row_dict.get('_mongo_id')
            unique_value = row_dict.get(unique_field)

            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø³Ø­Ø§Ø¨Ø©
            cloud_data = self._prepare_data_for_cloud(row_dict)

            try:
                if mongo_id:
                    # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ù…ÙˆØ¬ÙˆØ¯
                    from bson import ObjectId
                    collection.update_one(
                        {'_id': ObjectId(mongo_id)},
                        {'$set': cloud_data}
                    )
                else:
                    # ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø±Ø§Ø¬
                    existing = None
                    if unique_value:
                        existing = collection.find_one({unique_field: unique_value})

                    if existing:
                        # Ø±Ø¨Ø· Ø¨Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
                        mongo_id = str(existing['_id'])
                        collection.update_one(
                            {'_id': existing['_id']},
                            {'$set': cloud_data}
                        )
                    else:
                        # Ø¥Ø¯Ø±Ø§Ø¬ Ø¬Ø¯ÙŠØ¯
                        result = collection.insert_one(cloud_data)
                        mongo_id = str(result.inserted_id)

                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠ
                cursor.execute(
                    f"UPDATE {table_name} SET _mongo_id = ?, sync_status = 'synced' WHERE id = ?",
                    (mongo_id, local_id)
                )
                pushed += 1

            except Exception as e:
                logger.error(f"âŒ ÙØ´Ù„ Ø±ÙØ¹ {table_name}/{local_id}: {e}")

        conn.commit()
        if pushed > 0:
            logger.info(f"ğŸ“¤ {table_name}: Ø±ÙØ¹ {pushed} Ø³Ø¬Ù„")

    def _prepare_data_for_cloud(self, data: dict) -> dict:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø±ÙØ¹ Ù„Ù„Ø³Ø­Ø§Ø¨Ø©"""
        clean = {k: v for k, v in data.items()
                 if k not in ['id', '_mongo_id', 'sync_status']}

        # âš¡ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ logo_data
        # Ø¥Ø°Ø§ ÙƒØ§Ù† logo_data ÙØ§Ø±Øº Ùˆ logo_path ÙØ§Ø±Øº = Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­Ø°Ù Ø§Ù„ØµÙˆØ±Ø© ØµØ±Ø§Ø­Ø©
        # Ø¥Ø°Ø§ ÙƒØ§Ù† logo_data ÙØ§Ø±Øº Ùˆ logo_path Ù…ÙˆØ¬ÙˆØ¯ = Ù„Ø§ Ù†Ø±ÙŠØ¯ Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙˆÙ‚ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        logo_data_value = clean.get('logo_data', None)
        logo_path_value = clean.get('logo_path', None)
        
        if 'logo_data' in clean:
            if logo_data_value:
                # ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø© - Ø±ÙØ¹Ù‡Ø§ Ù„Ù„Ø³Ø­Ø§Ø¨Ø©
                logger.info(f"ğŸ“· Ø±ÙØ¹ logo_data ({len(logo_data_value)} Ø­Ø±Ù) Ù„Ù„Ø³Ø­Ø§Ø¨Ø©")
            elif not logo_path_value:
                # logo_data ÙØ§Ø±Øº Ùˆ logo_path ÙØ§Ø±Øº = Ø­Ø°Ù ØµØ±ÙŠØ­ Ù„Ù„ØµÙˆØ±Ø©
                clean['logo_data'] = ""  # Ø¥Ø±Ø³Ø§Ù„ Ù‚ÙŠÙ…Ø© ÙØ§Ø±ØºØ© ØµØ±ÙŠØ­Ø© Ù„Ù„Ø­Ø°Ù
                logger.info("ğŸ—‘ï¸ Ø­Ø°Ù logo_data Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø© (Ø­Ø°Ù ØµØ±ÙŠØ­)")
            else:
                # logo_data ÙØ§Ø±Øº Ù„ÙƒÙ† logo_path Ù…ÙˆØ¬ÙˆØ¯ = Ù„Ø§ Ù†Ø±ÙŠØ¯ Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙˆÙ‚ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
                del clean['logo_data']
                logger.debug("ğŸ“· ØªÙ… ØªØ¬Ø§Ù‡Ù„ logo_data Ø§Ù„ÙØ§Ø±Øº (Ù„Ù† ÙŠØªÙ… Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙˆÙ‚ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©)")
        
        if 'logo_path' in clean and not clean['logo_path']:
            # Ø¥Ø°Ø§ ÙƒØ§Ù† logo_path ÙØ§Ø±ØºØŒ Ù†Ø±Ø³Ù„ Ù‚ÙŠÙ…Ø© ÙØ§Ø±ØºØ© ØµØ±ÙŠØ­Ø©
            clean['logo_path'] = ""

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        for field in ['created_at', 'last_modified', 'date', 'issue_date',
                     'due_date', 'expiry_date', 'start_date', 'end_date']:
            if field in clean and clean[field]:
                try:
                    if isinstance(clean[field], str):
                        clean[field] = datetime.fromisoformat(
                            clean[field].replace('Z', '+00:00')
                        )
                except (ValueError, TypeError):
                    pass

        # ØªØ­ÙˆÙŠÙ„ JSON strings Ø¥Ù„Ù‰ objects
        for field in ['items', 'lines', 'data', 'milestones']:
            if field in clean and clean[field]:
                try:
                    if isinstance(clean[field], str):
                        clean[field] = json.loads(clean[field])
                except (json.JSONDecodeError, TypeError):
                    pass

        return clean

    def _sync_users_from_cloud(self):
        """Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡ (Ù…Ù† ÙˆØ¥Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©)"""
        try:
            cursor = self.repo.sqlite_cursor
            conn = self.repo.sqlite_conn

            # === 1. Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠÙŠÙ† Ø§Ù„Ø¬Ø¯Ø¯/Ø§Ù„Ù…Ø¹Ø¯Ù„ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø© ===
            logger.info("ğŸ“¤ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©...")
            cursor.execute("""
                SELECT * FROM users 
                WHERE sync_status IN ('new_offline', 'modified_offline', 'pending')
                   OR _mongo_id IS NULL
            """)
            local_pending = cursor.fetchall()

            uploaded_count = 0
            for row in local_pending:
                user_data = dict(row)
                username = user_data.get('username')
                local_id = user_data.get('id')

                existing_cloud = self.repo.mongo_db.users.find_one({'username': username})

                if existing_cloud:
                    mongo_id = str(existing_cloud['_id'])
                    update_data = {
                        'full_name': user_data.get('full_name'),
                        'email': user_data.get('email'),
                        'role': user_data.get('role'),
                        'is_active': bool(user_data.get('is_active', 1)),
                        'last_modified': datetime.now()
                    }
                    if user_data.get('password_hash'):
                        update_data['password_hash'] = user_data['password_hash']

                    self.repo.mongo_db.users.update_one(
                        {'_id': existing_cloud['_id']},
                        {'$set': update_data}
                    )
                    cursor.execute(
                        "UPDATE users SET _mongo_id=?, sync_status='synced' WHERE id=?",
                        (mongo_id, local_id)
                    )
                    uploaded_count += 1
                else:
                    new_user = {
                        'username': username,
                        'password_hash': user_data.get('password_hash'),
                        'full_name': user_data.get('full_name'),
                        'email': user_data.get('email'),
                        'role': user_data.get('role', 'sales'),
                        'is_active': bool(user_data.get('is_active', 1)),
                        'created_at': datetime.now(),
                        'last_modified': datetime.now()
                    }
                    result = self.repo.mongo_db.users.insert_one(new_user)
                    mongo_id = str(result.inserted_id)
                    cursor.execute(
                        "UPDATE users SET _mongo_id=?, sync_status='synced' WHERE id=?",
                        (mongo_id, local_id)
                    )
                    uploaded_count += 1

            if uploaded_count > 0:
                conn.commit()
                logger.info(f"ğŸ“¤ ØªÙ… Ø±ÙØ¹ {uploaded_count} Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ø³Ø­Ø§Ø¨Ø©")

            # === 2. ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø© ===
            logger.info("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©...")
            cloud_users = list(self.repo.mongo_db.users.find())
            if not cloud_users:
                return

            downloaded_count = 0
            for u in cloud_users:
                mongo_id = str(u['_id'])
                username = u.get('username')

                for field in ['created_at', 'last_modified', 'last_login']:
                    if field in u and hasattr(u[field], 'isoformat'):
                        u[field] = u[field].isoformat()

                cursor.execute(
                    "SELECT id, sync_status FROM users WHERE _mongo_id = ? OR username = ?",
                    (mongo_id, username)
                )
                exists = cursor.fetchone()

                if exists:
                    if exists[1] not in ('modified_offline', 'new_offline'):
                        cursor.execute("""
                            UPDATE users SET
                                full_name=?, email=?, role=?, is_active=?,
                                password_hash=?, _mongo_id=?, sync_status='synced',
                                last_modified=?
                            WHERE id=?
                        """, (
                            u.get('full_name'), u.get('email'), u.get('role'),
                            u.get('is_active', 1), u.get('password_hash'),
                            mongo_id, u.get('last_modified', datetime.now().isoformat()),
                            exists[0]
                        ))
                        downloaded_count += 1
                else:
                    cursor.execute("""
                        INSERT INTO users (
                            _mongo_id, username, full_name, email, role,
                            password_hash, is_active, sync_status, created_at, last_modified
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, 'synced', ?, ?)
                    """, (
                        mongo_id, username, u.get('full_name'), u.get('email'),
                        u.get('role'), u.get('password_hash'), u.get('is_active', 1),
                        u.get('created_at', datetime.now().isoformat()),
                        u.get('last_modified', datetime.now().isoformat())
                    ))
                    downloaded_count += 1

            conn.commit()
            logger.info(f"âœ… ØªÙ… Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† (Ø±ÙØ¹: {uploaded_count}, ØªÙ†Ø²ÙŠÙ„: {downloaded_count})")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: {e}")

    # ==========================================
    # Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    # ==========================================

    def remove_duplicates(self, table_name: str | None = None) -> dict[str, int]:
        """
        Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        ÙŠØ­ØªÙØ¸ Ø¨Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø£Ù‚Ø¯Ù… (Ø£Ù‚Ù„ id) ÙˆÙŠØ­Ø°Ù Ø§Ù„Ø¨Ø§Ù‚ÙŠ
        """
        tables = [table_name] if table_name else self.TABLES
        results = {}

        cursor = self.repo.sqlite_cursor
        conn = self.repo.sqlite_conn

        for table in tables:
            try:
                unique_field = self.UNIQUE_FIELDS.get(table, 'name')
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
                cursor.execute(f"""
                    SELECT {unique_field}, COUNT(*) as cnt, MIN(id) as keep_id
                    FROM {table}
                    WHERE {unique_field} IS NOT NULL
                    GROUP BY {unique_field}
                    HAVING cnt > 1
                """)
                duplicates = cursor.fetchall()

                deleted = 0
                for dup in duplicates:
                    unique_value = dup[0]
                    keep_id = dup[2]

                    # Ø­Ø°Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª (Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø£Ù‚Ø¯Ù…)
                    cursor.execute(f"""
                        DELETE FROM {table}
                        WHERE {unique_field} = ? AND id != ?
                    """, (unique_value, keep_id))
                    deleted += cursor.rowcount

                conn.commit()
                results[table] = deleted

                if deleted > 0:
                    logger.info(f"ğŸ—‘ï¸ {table}: Ø­Ø°Ù {deleted} Ø³Ø¬Ù„ Ù…ÙƒØ±Ø±")

            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø±Ø§Øª {table}: {e}")
                results[table] = 0

        return results

    def force_full_resync(self) -> dict[str, Any]:
        """
        Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø²Ø§Ù…Ù†Ø© ÙƒØ§Ù…Ù„Ø© Ù‚Ø³Ø±ÙŠØ©
        1. Ø­Ø°Ù ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        2. Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        """
        if not self.is_online:
            return {'success': False, 'reason': 'offline'}

        logger.warning("âš ï¸ Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø§Ù„Ù‚Ø³Ø±ÙŠØ©...")

        cursor = self.repo.sqlite_cursor
        conn = self.repo.sqlite_conn

        # Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© (Ù…Ø§ Ø¹Ø¯Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†)
        for table in self.TABLES:
            try:
                cursor.execute(f"DELETE FROM {table}")
                logger.info(f"ğŸ—‘ï¸ ØªÙ… Ù…Ø³Ø­ {table}")
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø³Ø­ {table}: {e}")

        conn.commit()

        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
        return self.full_sync_from_cloud()

    def get_sync_status(self) -> dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"""
        cursor = self.repo.sqlite_cursor
        status = {
            'is_online': self.is_online,
            'is_syncing': self._is_syncing,
            'tables': {}
        }

        for table in self.TABLES:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                total = cursor.fetchone()[0]

                cursor.execute(f"""
                    SELECT COUNT(*) FROM {table}
                    WHERE sync_status != 'synced' OR sync_status IS NULL
                """)
                pending = cursor.fetchone()[0]

                status['tables'][table] = {
                    'total': total,
                    'pending': pending,
                    'synced': total - pending
                }
            except Exception:
                status['tables'][table] = {'total': 0, 'pending': 0, 'synced': 0}

        return status


def create_unified_sync_manager(repository) -> UnifiedSyncManager:
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯ÙŠØ± Ù…Ø²Ø§Ù…Ù†Ø© Ù…ÙˆØ­Ø¯"""
    return UnifiedSyncManagerV3(repository)


    def remove_cloud_duplicates(self) -> dict[str, int]:
        """
        Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† MongoDB
        ÙŠØ­ØªÙØ¸ Ø¨Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø£Ù‚Ø¯Ù… (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ created_at)
        """
        if not self.is_online:
            return {}

        results = {}
        logger.info("ğŸ§¹ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©...")

        for table in self.TABLES:
            try:
                deleted = self._remove_cloud_table_duplicates(table)
                results[table] = deleted
                if deleted > 0:
                    logger.info(f"ğŸ—‘ï¸ {table}: Ø­Ø°Ù {deleted} Ø³Ø¬Ù„ Ù…ÙƒØ±Ø± Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©")
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ {table} Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨Ø©: {e}")
                results[table] = 0

        return results

    def _remove_cloud_table_duplicates(self, table_name: str) -> int:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø­Ø¯ ÙÙŠ MongoDB"""
        unique_field = self.UNIQUE_FIELDS.get(table_name, 'name')
        collection = self.repo.mongo_db[table_name]

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… aggregation
        pipeline = [
            {"$group": {
                "_id": f"${unique_field}",
                "count": {"$sum": 1},
                "docs": {"$push": {"_id": "$_id", "created_at": "$created_at"}}
            }},
            {"$match": {"count": {"$gt": 1}}}
        ]

        duplicates = list(collection.aggregate(pipeline))
        deleted = 0

        for dup in duplicates:
            docs = dup['docs']
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ created_at (Ø§Ù„Ø£Ù‚Ø¯Ù… Ø£ÙˆÙ„Ø§Ù‹)
            docs.sort(key=lambda x: x.get('created_at') or datetime.min)

            # Ø­Ø°Ù ÙƒÙ„ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ù…Ø§ Ø¹Ø¯Ø§ Ø§Ù„Ø£ÙˆÙ„
            for doc in docs[1:]:
                collection.delete_one({'_id': doc['_id']})
                deleted += 1

        return deleted

    def full_cleanup_and_sync(self) -> dict[str, Any]:
        """
        ØªÙ†Ø¸ÙŠÙ ÙƒØ§Ù…Ù„ ÙˆÙ…Ø²Ø§Ù…Ù†Ø©:
        1. ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ù† MongoDB
        2. ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        3. Ù…Ø²Ø§Ù…Ù†Ø© ÙƒØ§Ù…Ù„Ø©
        """
        results = {
            'cloud_cleanup': {},
            'local_cleanup': {},
            'sync': {}
        }

        if self.is_online:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
            logger.info("â˜ï¸ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©...")
            results['cloud_cleanup'] = self.remove_cloud_duplicates()

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­Ù„ÙŠ
        logger.info("ğŸ’¾ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©...")
        results['local_cleanup'] = self.remove_duplicates()

        # Ù…Ø²Ø§Ù…Ù†Ø©
        if self.is_online:
            logger.info("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©...")
            results['sync'] = self.full_sync_from_cloud()

        return results
